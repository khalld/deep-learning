{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bD9cZfJOuBS"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uVwMz27kyG1F"
      },
      "outputs": [],
      "source": [
        "# from libs.Dataset import *\n",
        "# from libs.SiameseNetwork import *\n",
        "import matplotlib.pyplot as plt\n",
        "import dill # allow to save notebook session, useful for test https://towardsdatascience.com/how-to-restore-your-jupyter-notebook-session-dfeadbd86d65\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "from pytorch_lightning.callbacks import Callback, progress\n",
        "from torch.optim import SGD\n",
        "from torch import nn\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import faiss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Mayw3PYEOuBU"
      },
      "outputs": [],
      "source": [
        "# For local execution\n",
        "data_dir = \"dataset/\"\n",
        "path_gdrive = \"\"\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "img_size = 96"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZoP0awGyG1I"
      },
      "source": [
        "### Loading Triplet Trashbin Pytorch Lighting Data Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "_353JJylyG1I",
        "outputId": "e02d391f-df64-41c6-d508-b489181f58e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Do nothing on prepare_data\n",
            "---Enter in setup---\n",
            "***fit***\n",
            "***test***\n"
          ]
        }
      ],
      "source": [
        "# Init our data pipeline\n",
        "dm = TripletTrashbinDataModule(data_dir=data_dir,  path_gdrive=path_gdrive, img_size=img_size)\n",
        "# To access the x_dataloader we need to call prepare_data and setup.\n",
        "dm.prepare_data()\n",
        "dm.setup()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5dggldsXyG1J",
        "outputId": "ade7bea1-ac0e-46e3-a13c-0b275a3b4001"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 172, in default_collate\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 172, in <listcomp>\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 138, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [3, 170, 96] at entry 0 and [3, 96, 170] at entry 3\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[1;32m/Users/danilo/GitHub/deep-learning/main-train-64x64.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/danilo/GitHub/deep-learning/main-train-64x64.ipynb#ch0000014?line=0'>1</a>\u001b[0m val_samples \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(dm\u001b[39m.\u001b[39;49mval_dataloader()))\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danilo/GitHub/deep-learning/main-train-64x64.ipynb#ch0000014?line=1'>2</a>\u001b[0m val_imgs, val_labels \u001b[39m=\u001b[39m val_samples[\u001b[39m0\u001b[39m], val_samples[\u001b[39m1\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/danilo/GitHub/deep-learning/main-train-64x64.ipynb#ch0000014?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRequired shape: val_imgs: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m val_shapes: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(val_imgs\u001b[39m.\u001b[39mshape, val_labels\u001b[39m.\u001b[39mshape))\n",
            "File \u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=527'>528</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=528'>529</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[0;32m--> <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=529'>530</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=530'>531</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=531'>532</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=532'>533</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=533'>534</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1224\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=1221'>1222</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=1222'>1223</a>\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=1223'>1224</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
            "File \u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1250\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=1247'>1248</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=1248'>1249</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=1249'>1250</a>\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/dataloader.py?line=1250'>1251</a>\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
            "File \u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/_utils.py:457\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/_utils.py?line=452'>453</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/_utils.py?line=453'>454</a>\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/_utils.py?line=454'>455</a>\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/_utils.py?line=455'>456</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> <a href='file:///Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/_utils.py?line=456'>457</a>\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n    return self.collate_fn(data)\n  File \"/Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 172, in default_collate\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 172, in <listcomp>\n    return [default_collate(samples) for samples in transposed]  # Backwards compatibility.\n  File \"/Users/danilo/GitHub/deep-learning/venv/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py\", line 138, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: stack expects each tensor to be equal size, but got [3, 170, 96] at entry 0 and [3, 96, 170] at entry 3\n"
          ]
        }
      ],
      "source": [
        "val_samples = next(iter(dm.val_dataloader()))\n",
        "val_imgs, val_labels = val_samples[0], val_samples[1]\n",
        "print(\"Required shape: val_imgs: {} val_shapes: {}\".format(val_imgs.shape, val_labels.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1KY1yjkyG1J"
      },
      "source": [
        "Print Train Dataset with matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWOi0NjoyG1J",
        "outputId": "e0a474b8-006a-466b-9b1a-82bdc36f544f"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(16,4))\n",
        "for i, idx in enumerate(np.random.choice(len(dm.trb_train), 6)):\n",
        "    ax = plt.subplot(3,10,i+1)\n",
        "    ax.set_title('Tripla {}'.format(idx))\n",
        "    \n",
        "    plt.imshow(dm.trb_train[i][0].T)\n",
        "    plt.subplot(3,10,i+11)\n",
        "    # plt.title('Class: {}'.format(dm.trb_train[i][3]))\n",
        "    plt.imshow(dm.trb_train[i][1].T)\n",
        "    # plt.title('Class: {}'.format(dm.trb_train[i][4]))\n",
        "    plt.subplot(3,10,i+21)\n",
        "    plt.imshow(dm.trb_train[i][2].T)\n",
        "    # plt.title('Class: {}'.format(dm.trb_train[i][5]))\n",
        "\n",
        "plt.subplots_adjust(hspace=0.1)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uos1N66LyG1M"
      },
      "source": [
        "### Test pretrained squeezeNet v2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FS3txw-CyG1M",
        "outputId": "a7bd3afa-e9b7-4f7e-e07b-81cacb8c6a51"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import squeezenet1_1\n",
        "\n",
        "squezenet_fe = squeezenet1_1(pretrained=True)\n",
        "\n",
        "# rimuovo classificatore finale, imposto il modulo identitÃ  al posto del classificatore\n",
        "squezenet_fe.classifier = nn.Identity()\n",
        "\n",
        "print(\"Dimensione del vettore di feature estratto per immagine {} -> {}\".format(torch.zeros(1,3,64,64).shape, squezenet_fe(torch.zeros(1,3,64,64)).shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVVU3LWqyG1N"
      },
      "outputs": [],
      "source": [
        "triplet_squeezenet1_1 = TripletNetworkTask(squezenet_fe)\n",
        "logger = TensorBoardLogger(\"metric_logs\", name=\"siamese_squeezenet1_1_64\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Aggiungi test delle performance non ancora ottimizzate con questo modello!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TSNE\n",
        "\n",
        "def extract_representations(model, loader, device):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    representations, labels = [], []\n",
        "    for batch in loader:\n",
        "        x = batch[0].to(device)\n",
        "        rep = model(x)\n",
        "        rep = rep.detach().to(device).numpy()\n",
        "        labels.append(batch[1])\n",
        "        representations.append(rep)\n",
        "    return np.concatenate(representations), np.concatenate(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dm.test_dataloader()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_rep, test_labels = extract_representations(triplet_squeezenet1_1.embedding_net, dm.test_dataloader(), device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "selected_rep = np.random.choice(len(test_rep), 1000 )\n",
        "selected_test_rep = test_rep[selected_rep]\n",
        "selected_test_lab = test_labels[selected_rep]\n",
        "\n",
        "tsne = TSNE(2)\n",
        "rep_tsne = tsne.fit_transform(selected_test_rep)\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "for c in np.unique(selected_test_lab):\n",
        "    print(c)\n",
        "    print(rep_tsne[0,0])\n",
        "    print(rep_tsne[c, 0])\n",
        "    print(rep_tsne[selected_test_lab == c, 0])\n",
        "    print(rep_tsne[selected_test_lab == c, 1])\n",
        "    plt.plot(rep_tsne[selected_test_lab == c, 0], rep_tsne[selected_test_lab==c, 1], 'o', label=c)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAsbvz2jyG1N",
        "outputId": "10e8df8a-29fd-40ee-e2da-523b0d1b8a0a"
      },
      "outputs": [],
      "source": [
        "trainer = pl.Trainer(gpus=0,\n",
        "                    max_epochs=1,\n",
        "                    callbacks=[progress.TQDMProgressBar()],\n",
        "                    logger=logger,\n",
        "                    accelerator=\"auto\",\n",
        "                    )\n",
        "\n",
        "trainer.fit(model=triplet_squeezenet1_1, datamodule=dm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwUCelBuyG1N",
        "outputId": "1d07fcb8-555d-4953-ca5d-e22b6bd607ae"
      },
      "outputs": [],
      "source": [
        "loaded_trainer = pl.Trainer(\n",
        "    gpus=0,\n",
        "    logger=logger,\n",
        "    max_epochs=2,\n",
        "    progress_bar_refresh_rate=0,\n",
        "    accelerator=\"auto\",\n",
        "    resume_from_checkpoint=\"\"\n",
        "    )\n",
        "\n",
        "\n",
        "# TODO:\n",
        "loaded_trainer.fit(model=triplet_squeezenet1_1, datamodule=dm, ckpt_path='metric_logs/siamese_squeezenet1_1_64/version_1/checkpoints/epoch=0-step=206.ckpt')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "7b5afb9c6c69ce826c7b3420d962c361055e44e7f6b101c54e3065067bcff4ff"
    },
    "kernelspec": {
      "display_name": "Python 3.9.9 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
