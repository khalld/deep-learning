{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from libs.code import *\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from tqdm import tqdm\n",
    "# ------\n",
    "# to export in ipynb\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "# from libs.code import *\n",
    "from libs.Dataset import *\n",
    "from libs.PretrainedModels import *\n",
    "# from libs.VAE import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1996)\n",
    "np.random.seed(1996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DST = 'dataset/all_labels.csv'\n",
    "PATH_GDRIVE = ''\n",
    "NUM_WORKERS = 0 # evita il warning\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "GPUS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline nearest neighbor su RGB\n",
    "def extract_rgb_representations(loader):\n",
    "    representations, labels = [], []\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        representations.append(batch[0].view(batch[0].shape[0],-1).numpy())\n",
    "        labels.append(batch[1])\n",
    "\n",
    "    return np.concatenate(representations), np.concatenate(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valori pretrained\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225] \n",
    "\n",
    "dataset = TrashbinDataset(csv=PATH_DST, transform=transforms.Compose([transforms.Resize((32,32)), transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)]))\n",
    "\n",
    "dataset_train, dataset_test = split_into_train_and_test(dataset)\n",
    "\n",
    "dst_train_loader = DataLoader(dataset_train, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dst_test_loader = DataLoader(dataset_test, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rgb_representations(loader):\n",
    "    representations, labels = [], []\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        representations.append(batch[0].view(batch[0].shape[0],-1).numpy())\n",
    "        labels.append(batch[1])\n",
    "\n",
    "    return np.concatenate(representations), np.concatenate(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330/330 [05:59<00:00,  1.09s/it]\n",
      "100%|██████████| 83/83 [01:29<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "dst_train_rep_rgb, dst_train_labels = extract_rgb_representations(loader=dst_train_loader)\n",
    "dst_test_rep_rgb, dst_test_labels = extract_rgb_representations(loader=dst_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10560, 150528)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rappresentazioni di training\n",
    "dst_train_rep_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "def predict_nn(train_rep, test_rep, train_label):\n",
    "    \"\"\" Funzione che permette di predire le etichette sul test set analizzando l'algoritmo NN. !pip install faiss-gpu/cpu\"\"\"\n",
    "    # inizializzo l'oggetto index utilizzato x indicizzare le rappresentazioni\n",
    "    index = faiss.IndexFlat(train_rep.shape[1])\n",
    "    # aggiungo le rappresentazioni di training all'indice\n",
    "    index.add(train_rep.astype(np.float32))\n",
    "    # effettuiamo la ricerca\n",
    "\n",
    "    indices = np.array([index.search(x.reshape(1,-1).astype(np.float32), k=1)[1][0][0] for x in test_rep])\n",
    "\n",
    "    #restituisco le etichette predette\n",
    "    return train_label[indices].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ottengo le predizionis ul test set:\n",
    "pred_test_label_rgb = predict_nn(dst_train_rep_rgb, dst_test_rep_rgb, dst_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, ..., 1, 2, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_label_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(pred_label, ground_truth):\n",
    "    \"\"\" Valuto la bontà delle predizioni ottenute calcolando la distanza euclidea tra il vettore di label\n",
    "        predetto e quelli di ground truth\"\"\"\n",
    "    dist = np.sqrt(np.sum(np.square(pred_label-ground_truth)))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valuto le performance della baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification error: 5.48\n"
     ]
    }
   ],
   "source": [
    "classification_error = evaluate_classification(pred_test_label_rgb, dst_test_labels)\n",
    "\n",
    "print(f\"Classification error: {classification_error:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1280])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # training della funzione di rappresentazione mediante triplet\n",
    "# # TODO: importa la migliore che avevi fatto dagli studi precedenti\n",
    "\n",
    "# from torchvision.models import mobilenet_v2\n",
    "# base_model = mobilenet_v2()\n",
    "\n",
    "# # voglio usare il modello per estrarre le featuer, quindi devo rimuovere il classificatore finale. nello specifico imposto il classificatore a un modulo identità\n",
    "# base_model.classifier = nn.Identity()\n",
    "# # verifico qual è la dimensione del vettore di feature estratto per una immagine di input di shape 3 x 244 x 224\n",
    "# base_model(torch.zeros(1,3,244,244)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting: bool):\n",
    "    \"\"\"Helper function that sets the `require_grad` attribute of parameter in the model to False when is used feature extracting\"\"\"\n",
    "\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/danilo/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 86528])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PRETRAINED_MODEL_PATH =  'models/squeezeNet_pretrained.pth'\n",
    "CLASSES = 3\n",
    "\n",
    "squeezeNet_1_0 = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)\n",
    "squeezeNet_1_0.classifier[1] = nn.Conv2d(512, CLASSES, kernel_size=(1,1), stride=(1,1))\n",
    "squeezeNet_1_0.num_classes = CLASSES\n",
    "squeezeNet_1_0.load_state_dict(torch.load(PRETRAINED_MODEL_PATH))\n",
    "#model.eval() ?\n",
    "# set_parameters qua ?? Non penso.\n",
    "\n",
    "squeezeNet_1_0.classifier = nn.Identity()\n",
    "\n",
    "squeezeNet_1_0(torch.zeros(1, 3, 224,224)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# il modello estrae vettori di rappresentazione di 1280 unità. definisco una funzione per estrarre le rappresentazioni di dataloader di training e test\n",
    "\n",
    "def extract_rep_(model, loader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    representations, labels = [], []\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        x = batch[0].to(device)\n",
    "        # print(x.shape)\n",
    "        # print(x.size)\n",
    "        # break\n",
    "        rep = model(x)\n",
    "        rep = rep.detach().to('cpu').numpy()\n",
    "        labels.append(batch[1])\n",
    "        representations.append(rep)\n",
    "    \n",
    "    return np.concatenate(representations), np.concatenate(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "dataset = TrashbinDataset(csv=PATH_DST, transform=transforms.Compose([\n",
    "    transforms.Resize((224,224)), \n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "    ]))\n",
    "\n",
    "dataset_train, dataset_test = split_into_train_and_test(dataset)\n",
    "\n",
    "dst_train_loader = DataLoader(dataset_train, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dst_test_loader = DataLoader(dataset_test, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330/330 [13:38<00:00,  2.48s/it]\n",
      "100%|██████████| 83/83 [03:17<00:00,  2.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# uso il modello non ancora allenato per estrarre le rappresentazioni dal training e dal test set\n",
    "dst_train_rep_base, dst_train_labels = extract_rep_(squeezeNet_1_0, dst_train_loader)\n",
    "dst_test_rep_base, dst_test_labels = extract_rep_(squeezeNet_1_0, dst_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valuto le performance del sistema con rappresentazioni non ancora ottimizzate\n",
    "\n",
    "# forse gli import sono sbagliati TODO: sistemali\n",
    "pred_test_label_base = predict_nn(dst_train_rep_rgb, dst_test_rep_rgb, dst_train_labels)\n",
    "classification_error = evaluate_classification(pred_test_label_base, dst_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.477225575051661"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_dst = TripletTrashbin(root=PATH_DST, transform=transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)\n",
    "    ]))\n",
    "\n",
    "triplet_dst_train, triplet_dst_test = split_into_train_and_test(triplet_dst)\n",
    "\n",
    "triplet_dataset_train_loader = DataLoader(triplet_dst_train, num_workers=0, batch_size=BATCH_SIZE, shuffle=True)\n",
    "triplet_dataset_test_loader = DataLoader(triplet_dst_test, num_workers=0, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name          | Type              | Params\n",
      "----------------------------------------------------\n",
      "0 | embedding_net | SqueezeNet        | 735 K \n",
      "1 | criterion     | TripletMarginLoss | 0     \n",
      "----------------------------------------------------\n",
      "735 K     Trainable params\n",
      "0         Non-trainable params\n",
      "735 K     Total params\n",
      "2.942     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilo/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:117: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilo/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:117: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  58%|█████▊    | 192/330 [37:32<26:58, 11.73s/it, loss=nan, v_num=0]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilo/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:686: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "from libs.SiameseNetwork import TripletNetworkTask\n",
    "\n",
    "triplet_trashbin_task =  TripletNetworkTask(squeezeNet_1_0, lr=0.002)\n",
    "logger = TensorBoardLogger(\"metric_logs\", name=\"test_trashbin_v1\")\n",
    "\n",
    "trainer = pl.Trainer(gpus=GPUS, logger = logger, max_epochs = 10, check_val_every_n_epoch=5)\n",
    "\n",
    "trainer.fit(triplet_trashbin_task, triplet_dataset_train_loader, triplet_dataset_test_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b5afb9c6c69ce826c7b3420d962c361055e44e7f6b101c54e3065067bcff4ff"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
