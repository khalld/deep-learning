{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from libs.code import *\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from tqdm import tqdm\n",
    "# ------\n",
    "# to export in ipynb\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "import pytorch_lightning as pl\n",
    "# from libs.code import *\n",
    "from libs.Dataset import *\n",
    "# from libs.VAE import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DST = 'dataset/all_labels.csv'\n",
    "PATH_GDRIVE = ''\n",
    "NUM_WORKERS = 8 # ricordati se significava tutto o niente\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "GPUS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline nearest neighbor su RGB\n",
    "def extract_rgb_representations(loader):\n",
    "    representations, labels = [], []\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        representations.append(batch[0].view(batch[0].shape[0],-1).numpy())\n",
    "        labels.append(batch[1])\n",
    "\n",
    "    return np.concatenate(representations), np.concatenate(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "dataset = TrashbinDataset(csv=PATH_DST, transform=transforms.Compose([transforms.Resize((32,32)), transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)]))\n",
    "\n",
    "dataset_train, dataset_test = split_into_train_and_test(dataset)\n",
    "\n",
    "dst_train_loader = DataLoader(dataset_train, shuffle=True, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE)\n",
    "dst_test_loader = DataLoader(dataset_test, shuffle=True, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330/330 [00:54<00:00,  6.01it/s]\n",
      "100%|██████████| 83/83 [00:14<00:00,  5.77it/s]\n"
     ]
    }
   ],
   "source": [
    "dst_train_rep_rgb, dst_train_labels = extract_rgb_representations(loader=dst_train_loader)\n",
    "dst_test_rep_rgb, dst_test_labels = extract_rgb_representations(loader=dst_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10560, 3072)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rappresentazioni di training\n",
    "dst_train_rep_rgb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "def predict_nn(train_rep, test_rep, train_label):\n",
    "    \"\"\" Funzione che permette di predire le etichette sul test set analizzando l'algoritmo NN. !pip install faiss-gpu/cpu\"\"\"\n",
    "    # inizializzo l'oggetto index utilizzato x indicizzare le rappresentazioni\n",
    "    index = faiss.IndexFlat(train_rep.shape[1])\n",
    "    # aggiungo le rappresentazioni di training all'indice\n",
    "    index.add(train_rep.astype(np.float32))\n",
    "    # effettuiamo la ricerca\n",
    "\n",
    "    indices = np.array([index.search(x.reshape(1,-1).astype(np.float32), k=1)[1][0][0] for x in test_rep])\n",
    "\n",
    "    #restituisco le etichette predette\n",
    "    return train_label[indices].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ottengo le predizionis ul test set:\n",
    "pred_test_label_rgb = predict_nn(dst_train_rep_rgb, dst_test_rep_rgb, dst_train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 1, ..., 1, 2, 2])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_label_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(pred_label, ground_truth):\n",
    "    \"\"\" Valuto la bontà delle predizioni ottenute calcolando la distanza euclidea tra il vettore di label\n",
    "        predetto e quelli di ground truth\"\"\"\n",
    "    dist = np.sqrt(np.sum(np.square(pred_label-ground_truth)))\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valuto le performance della baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification error: 1.00\n"
     ]
    }
   ],
   "source": [
    "classification_error = evaluate_classification(pred_test_label_rgb, dst_test_labels)\n",
    "\n",
    "print(f\"Classification error: {classification_error:0.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1280])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training della funzione di rappresentazione mediante triplet\n",
    "# TODO: importa la migliore che avevi fatto dagli studi precedenti\n",
    "\n",
    "from torchvision.models import mobilenet_v2\n",
    "base_model = mobilenet_v2()\n",
    "\n",
    "# voglio usare il modello per estrarre le featuer, quindi devo rimuovere il classificatore finale. nello specifico imposto il classificatore a un modulo identità\n",
    "base_model.classifier = nn.Identity()\n",
    "# verifico qual è la dimensione del vettore di feature estratto per una immagine di input di shape 3 x 244 x 224\n",
    "base_model(torch.zeros(1,3,244,244)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# il modello estrae vettori di rappresentazione di 1280 unità. definisco una funzione per estrarre le rappresentazioni di dataloader di training e test\n",
    "\n",
    "def extract_rep_(model, loader):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    representations, labels = [], []\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        x = batch[0].to(device)\n",
    "        # print(x.shape)\n",
    "        # print(x.size)\n",
    "        # break\n",
    "        rep = model(x)\n",
    "        rep = rep.detach().to('cpu').numpy()\n",
    "        labels.append(batch[1])\n",
    "        representations.append(rep)\n",
    "    \n",
    "    return np.concatenate(representations), np.concatenate(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "dataset = TrashbinDataset(csv=PATH_DST, transform=transforms.Compose([transforms.Resize((244,244)), transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)]))\n",
    "\n",
    "dataset_train, dataset_test = split_into_train_and_test(dataset)\n",
    "\n",
    "dst_train_loader = DataLoader(dataset_train, shuffle=True, num_workers=0, batch_size=32)\n",
    "dst_test_loader = DataLoader(dataset_test, shuffle=True, num_workers=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330/330 [16:48<00:00,  3.06s/it]\n",
      "100%|██████████| 83/83 [04:05<00:00,  2.96s/it]\n"
     ]
    }
   ],
   "source": [
    "# uso il modello non ancora allenato per estrarre le rappresentazioni dal training e dal test set\n",
    "dst_train_rep_base, dst_train_labels = extract_rep_(base_model, dst_train_loader)\n",
    "dst_test_rep_base, dst_test_labels = extract_rep_(base_model, dst_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# valuto le performance del sistema con rappresentazioni non ancora ottimizzate\n",
    "\n",
    "pred_test_label_base = predict_nn(dst_train_rep_rgb, dst_test_rep_rgb, dst_train_labels)\n",
    "classification_error = evaluate_classification(pred_test_label_base, dst_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58.61740355901138"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletTrashbin(data.Dataset):\n",
    "    def __init__(self, root = 'dataset/all_labels.csv', transform = None, path_gdrive='') -> None:\n",
    "#        super().__init__()\n",
    "        self.dataset = TrashbinDataset(root, transform=transform, path_gdrive=path_gdrive)\n",
    "        # self.dataset = self.dataset.data    # dipende dalla classe sopra, evito di chiamare un oggetto lungo\n",
    "        self.class_to_indices = [np.where(self.dataset.data.label == label)[0] for label in range(3)]  # N delle classi\n",
    "\n",
    "        self.generate_triplets()\n",
    "    \n",
    "    def generate_triplets(self):\n",
    "        \"\"\" Genera le triplete associando ongi elemento del dataset due nuovi elementi. Uno simile e uno dissimile\"\"\"\n",
    "\n",
    "        self.similar_idx = []\n",
    "        self.dissimilar_idx = []\n",
    "\n",
    "        # cu.printer_helper(\"Start making triplets...\")\n",
    "\n",
    "        for i in range(len(self.dataset)):\n",
    "            # classe del primo elemento della tripletta\n",
    "            c1 = self.dataset[i][1] # la classe la trovo sempre alla posizione 1 dato il dataset di sopra\n",
    "            # indice dell'elemento simile\n",
    "            j = np.random.choice(self.class_to_indices[c1])\n",
    "            # scelgo una classe diversa a caso\n",
    "            diff_class = np.random.choice(list(set(range(3))-{c1}))\n",
    "            # campiono dalla classe di ^ per ottenere l'indice dell'elemento dissimile\n",
    "            k = np.random.choice(self.class_to_indices[diff_class])\n",
    "\n",
    "            self.similar_idx.append(j)\n",
    "            self.dissimilar_idx.append(k)\n",
    "\n",
    "        # cu.printer_helper(\"Dataset loaded successfully!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        im1, l1 = self.dataset[index]\n",
    "        im2, l2 = self.dataset[self.similar_idx[index]]\n",
    "        im3, l3 = self.dataset[self.dissimilar_idx[index]]\n",
    "\n",
    "        return im1, im2, im3, l1, l2, l3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_trashbin = TripletTrashbin(root=PATH_DST, transform=transforms.Compose([transforms.Resize((244,244)), transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)]))\n",
    "\n",
    "triplet_dataset_train, triplet_dataset_test = split_into_train_and_test(triplet_trashbin)\n",
    "\n",
    "triplet_dataset_train_loader = DataLoader(triplet_dataset_train, shuffle=True, num_workers=0, batch_size=32)\n",
    "triplet_dataset_test_loader = DataLoader(triplet_dataset_test, num_workers=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNetworkTask(pl.LightningModule):\n",
    "    def __init__(self, embedding_net, lr=0.01, momentum=0.99, margin=2, num_class=3):\n",
    "        super(TripletNetworkTask, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.embedding_net = embedding_net\n",
    "        self.criterion = nn.TripletMarginLoss(margin=margin)\n",
    "        self.num_class = num_class\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return SGD(self.embedding_net.parameters(), self.hparams.lr, momentum=self.hparams.momentum)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        I_i, I_j, I_k, *_ = batch\n",
    "        phi_i = self.embedding_net(I_i)\n",
    "        phi_j = self.embedding_net(I_j)\n",
    "        phi_k = self.embedding_net(I_k)\n",
    "\n",
    "        # calcoliamo la loss\n",
    "        loss_triplet = self.criterion(phi_i, phi_j, phi_k)\n",
    "        \n",
    "        loss_embedd = phi_i.norm(2) + phi_i.norm(2) + phi_i.norm(2)\n",
    "        loss = loss_triplet + 0.001 *loss_embedd\n",
    "\n",
    "        self.log('train/loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        I_i, I_j, I_k, *_ = batch\n",
    "        phi_i = self.embedding_net(I_i)\n",
    "        phi_j = self.embedding_net(I_j)\n",
    "        phi_k = self.embedding_net(I_k)\n",
    "\n",
    "        #calcolo la loss\n",
    "        loss_triplet = self.criterion(phi_i, phi_j, phi_k)\n",
    "\n",
    "        loss_embedd = phi_i.norm(2) + phi_i.norm(2) + phi_i.norm(2)\n",
    "        loss = loss_triplet + 0.001 * loss_embedd\n",
    "\n",
    "        self.log('valid/loss', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name          | Type              | Params\n",
      "----------------------------------------------------\n",
      "0 | embedding_net | MobileNetV2       | 2.2 M \n",
      "1 | criterion     | TripletMarginLoss | 0     \n",
      "----------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.895     Total estimated model params size (MB)\n"
     ]
    }
   ],
   "source": [
    "triplet_trashbin_task =  TripletNetworkTask(base_model, lr=0.001)\n",
    "logger = TensorBoardLogger(\"metric_logs\", name=\"test_trashbin_v1\")\n",
    "\n",
    "trainer = pl.Trainer(gpus=GPUS, logger = logger, max_epochs = 10, check_val_every_n_epoch=5)\n",
    "\n",
    "trainer.fit(triplet_trashbin_task, triplet_dataset_train_loader, triplet_dataset_test_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b5afb9c6c69ce826c7b3420d962c361055e44e7f6b101c54e3065067bcff4ff"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
