{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from libs.code import *\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import faiss\n",
    "\n",
    "from libs.Dataset import *\n",
    "from libs.util import *\n",
    "from libs.SiameseNetwork import TripletNetworkTask\n",
    "# non necessari !\n",
    "# from libs.code import *\n",
    "# from libs.VAE import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imposto i seed e le variabili globali:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "random.seed(1996)\n",
    "np.random.seed(1996)\n",
    "pl.seed_everything(1996)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DST = 'dataset/all_labels.csv'\n",
    "PATH_GDRIVE = ''\n",
    "# TODO: se setto > 0 mi da \n",
    "# [W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
    "# e non mi permette di effettuare il training. tuttavia resta troppo lento. come procedo?\n",
    "NUM_WORKERS = 12\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "GPUS = 0\n",
    "PRETRAINED_MODEL_PATH =  'models/squeezeNet_pretrained.pth'\n",
    "num_class = 3\n",
    "\n",
    "# valori pretrained\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transf = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "from torch.nn import ModuleList\n",
    "\n",
    "transf_train = transforms.Compose([\n",
    "        transforms.Resize(230), # taglio solo una piccola parte col randomCrop in modo tale da prendere sempre il secchio\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomApply(ModuleList([\n",
    "            transforms.ColorJitter(brightness=.3, hue=.2),\n",
    "        ]), p=0.3),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.RandomHorizontalFlip(p=0.3),\n",
    "        transforms.RandomPerspective(distortion_scale=0.3, p=0.2),\n",
    "        transforms.RandomEqualize(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])\n",
    "\n",
    "\n",
    "transf_test = transforms.Compose([\n",
    "        transforms.Resize(256), \n",
    "        transforms.CenterCrop(224), \n",
    "        transforms.AutoAugment(transforms.AutoAugmentPolicy.SVHN),\n",
    "        transforms.RandomInvert(p=0.3),\n",
    "        transforms.RandomHorizontalFlip(p=0.2),\n",
    "        transforms.RandomGrayscale(p=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carico il dataset singolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train: 10560 , df_test: 2640, is splitted correctly: True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(PATH_DST)\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.20, random_state=0)\n",
    "\n",
    "print(\"df_train: {} , df_test: {}, is splitted correctly: {}\".format(len(df_train), len(df_test), (len(df) == (len(df_test)+len(df_train)) )))\n",
    "\n",
    "df_train.to_csv(\"dataset/df_training.csv\")\n",
    "df_test.to_csv(\"dataset/df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_train = TrashbinDataset(csv=PATH_DST, transform=transf_train)\n",
    "dst_test = TrashbinDataset(csv=PATH_DST, transform=transf_test)\n",
    "\n",
    "\n",
    "dst_train_loader = DataLoader(dst_train, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dst_test_loader = DataLoader(dst_test, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estrazione delle rappresentazioni e prime predizioni con nn\n",
    "Estraggo le rappresentazioni rgb dai loader: <b>TODO:</b> non ho ancora eseguito con la data augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330/330 [05:47<00:00,  1.05s/it]\n",
      "100%|██████████| 83/83 [01:25<00:00,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "dst_train_rep_rgb, dst_train_labels = extract_rgb_representations(loader=dst_train_loader)\n",
    "dst_test_rep_rgb, dst_test_labels = extract_rgb_representations(loader=dst_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappresentazioni di training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10560, 150528)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst_train_rep_rgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ottengo le predizioni sul test-set usando `predict_nn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample di label: [2 0 0 ... 0 2 2]\n"
     ]
    }
   ],
   "source": [
    "pred_test_label_rgb = predict_nn(dst_train_rep_rgb, dst_test_rep_rgb, dst_train_labels)\n",
    "print(f\"Sample di label: {pred_test_label_rgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valuto le performance della baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification error: 5.29\n"
     ]
    }
   ],
   "source": [
    "classification_error = evaluate_classification(pred_test_label_rgb, dst_test_labels)\n",
    "print(f\"Classification error: {classification_error:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modello"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<s>Importo per effettuare il training della triplenet il miglior modello trovato nella precedente relazione: `SqueezeNet v1.0`. Importo dunque i pesi già trovati dopo il training di 100 epoche .... <b>TODO migliora la descrizione</b> ... importo i pesi.. faccio le opportune modifiche ...</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/danilo/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Vorrei usare il modello già allenato precedentemente. Ma come?\n",
    "# scarico il modello da pytorch\n",
    "squeezeNet_1_0 = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)\n",
    "# applico le opportune modifiche\n",
    "squeezeNet_1_0.classifier[1] = nn.Conv2d(512, num_class, kernel_size=(1,1), stride=(1,1))\n",
    "# # # carico i pesi salvati\n",
    "\n",
    "squeezeNet_1_0.load_state_dict(torch.load(PRETRAINED_MODEL_PATH))\n",
    "\n",
    "# testo così\n",
    "squeezeNet_1_0.classifier = nn.Sequential(\n",
    "    # nn.Dropout(p=0.5, inplace=False),\n",
    "    # nn.Conv2d(512, num_class, kernel_size=(1, 1), stride=(1, 1)),\n",
    "    # nn.Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.AdaptiveAvgPool2d(output_size=(1, 1)),\n",
    "    nn.Identity()\n",
    "  )\n",
    "\n",
    "squeezeNet_1_0(torch.zeros(1, 3, 224,224)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330/330 [12:34<00:00,  2.29s/it]\n",
      "100%|██████████| 83/83 [03:09<00:00,  2.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# uso il modello, allenato nel precedente progetto, per estrarre le rappresentazioni dal training e dal test set\n",
    "dst_train_rep, dst_train_labels = extract_rep_squeezeNet(squeezeNet_1_0, dst_train_loader)\n",
    "dst_test_rep, dst_test_labels = extract_rep_squeezeNet(squeezeNet_1_0, dst_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valuto le performance del sistema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification error: 0.0\n"
     ]
    }
   ],
   "source": [
    "# valuto le performance del sistema con rappresentazioni non ancora ottimizzate\n",
    "pred_test_label = predict_nn(dst_train_rep, dst_test_rep, dst_train_labels)\n",
    "classification_error = evaluate_classification(pred_test_label, dst_test_labels)\n",
    "print(f\"Classification error: {classification_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carico il dataset in triplette:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "# non funzionano !!! non allenano il modello!\n",
    "\n",
    "# transf_train = transforms.Compose([\n",
    "#         transforms.Resize(230), # taglio solo una piccola parte col randomCrop in modo tale da prendere sempre il secchio\n",
    "#         transforms.RandomCrop(224),\n",
    "#         transforms.RandomApply(ModuleList([\n",
    "#             transforms.ColorJitter(brightness=.3, hue=.2),\n",
    "#         ]), p=0.3),\n",
    "#         transforms.RandomGrayscale(p=0.2),\n",
    "#         transforms.RandomHorizontalFlip(p=0.3),\n",
    "#         transforms.RandomPerspective(distortion_scale=0.3, p=0.2),\n",
    "#         transforms.RandomEqualize(p=0.2),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=mean, std=std)\n",
    "#     ])\n",
    "\n",
    "\n",
    "# transf_test = transforms.Compose([\n",
    "#         transforms.Resize(256), \n",
    "#         transforms.CenterCrop(224), \n",
    "#         transforms.AutoAugment(transforms.AutoAugmentPolicy.SVHN),\n",
    "#         transforms.RandomInvert(p=0.3),\n",
    "#         transforms.RandomHorizontalFlip(p=0.2),\n",
    "#         transforms.RandomGrayscale(p=0.2),\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=mean, std=std)\n",
    "#     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# dst_triplet = TripletTrashbin(root=PATH_DST, transform=transf)\n",
    "\n",
    "# dst_train_triplet, dst_test_triplet = split_into_train_and_test(dst_triplet)\n",
    "\n",
    "# carico direttamente gli stessi elementi\n",
    "dst_train_triplet = TripletTrashbin(root='dataset/df_training.csv', transform=transf)\n",
    "dst_test_triplet = TripletTrashbin(root='dataset/df_test.csv', transform=transf)\n",
    "\n",
    "triplet_dataset_train_loader = DataLoader(dst_train_triplet, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "triplet_dataset_test_loader = DataLoader(dst_test_triplet, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: mostra le immagini delle triplette\n",
    "\n",
    "# 7 minuti e qualcosa ricorda.... ^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primo training:\n",
    "Alleno la rete con lr=0.002 che è il migliore trovato per SqueezeNet nel precedente progetto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNetworkTaskDebugged(pl.LightningModule):\n",
    "    # lr uguale a quello del progetto vecchio\n",
    "    def __init__(self, embedding_net, lr=0.002, momentum=0.99, margin=2, num_class=3):\n",
    "        super(TripletNetworkTaskDebugged, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.embedding_net = embedding_net\n",
    "        self.criterion = nn.TripletMarginLoss(margin=margin)\n",
    "        self.num_class = num_class\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return SGD(self.embedding_net.parameters(), self.hparams.lr, momentum=self.hparams.momentum)\n",
    "\n",
    "    # Lightning automatically sets the model to training for training_step and to eval for validation.\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # print(\"STEP 0: \")\n",
    "\n",
    "        I_i, I_j, I_k, *_ = batch\n",
    "\n",
    "        # print(f\"i_i: {len(I_i)}, i_j :{len(I_j)}, i_k:{len(I_k)}\")\n",
    "\n",
    "        # print(f\"Shape: {I_i.shape}\")\n",
    "\n",
    "        phi_i = self.embedding_net(I_i)\n",
    "        phi_j = self.embedding_net(I_j)\n",
    "        phi_k = self.embedding_net(I_k)\n",
    "\n",
    "        # print(f\"phi_i: {phi_i}, phi_j :{phi_j}, phi_k:{phi_k}\")\n",
    "\n",
    "        # calcoliamo la loss\n",
    "        loss_triplet = self.criterion(phi_i, phi_j, phi_k)\n",
    "        # print(f\"training_step: loss_triplet {loss_triplet}\")\n",
    "        # self.log('train/loss', loss_triplet)\n",
    "        # return loss_triplet\n",
    "        \n",
    "        loss_embedd = phi_i.norm(2) + phi_i.norm(2) + phi_i.norm(2)\n",
    "\n",
    "        # print(f\"loss embedd {loss_embedd}\")\n",
    "\n",
    "        loss = loss_triplet + 0.001 *loss_embedd\n",
    "        \n",
    "        # print(f\"loss {loss}\")\n",
    "\n",
    "        self.log('train/loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        I_i, I_j, I_k, *_ = batch\n",
    "        phi_i = self.embedding_net(I_i)\n",
    "        phi_j = self.embedding_net(I_j)\n",
    "        phi_k = self.embedding_net(I_k)\n",
    "\n",
    "        #calcolo la loss\n",
    "        loss_triplet = self.criterion(phi_i, phi_j, phi_k)\n",
    "        # print(f\"validation_step: loss_triplet {loss_triplet}\")\n",
    "        # self.log('train/loss', loss_triplet)\n",
    "        # return loss_triplet\n",
    "        loss_embedd = phi_i.norm(2) + phi_i.norm(2) + phi_i.norm(2)\n",
    "        loss = loss_triplet + 0.001 * loss_embedd\n",
    "\n",
    "        self.log('valid/loss', loss)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name          | Type              | Params\n",
      "----------------------------------------------------\n",
      "0 | embedding_net | SqueezeNet        | 735 K \n",
      "1 | criterion     | TripletMarginLoss | 0     \n",
      "----------------------------------------------------\n",
      "735 K     Trainable params\n",
      "0         Non-trainable params\n",
      "735 K     Total params\n",
      "2.942     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   4%|▎         | 3/83 [03:06<1:23:01, 62.27s/it, loss=1.01, v_num=9]   "
     ]
    }
   ],
   "source": [
    "triplet_trashbin_task = TripletNetworkTaskDebugged(squeezeNet_1_0, lr=0.002)\n",
    "logger = TensorBoardLogger(\"metric_logs\", name=\"test_trashbin_v1\",)\n",
    "\n",
    "trainer = pl.Trainer(gpus=GPUS, logger = logger, max_epochs = 5, check_val_every_n_epoch = 2, )\n",
    "trainer.fit(triplet_trashbin_task, triplet_dataset_train_loader, triplet_dataset_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Secondo training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "Restoring states from the checkpoint path at metric_logs/test_trashbin_v1/version_0/checkpoints/epoch=9-step=3299.ckpt\n",
      "/Users/danilo/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:248: UserWarning: You're resuming from a checkpoint that ended mid-epoch. Training will start from the beginning of the next epoch. This can cause unreliable results if further training is done, consider using an end of epoch checkpoint.\n",
      "  \"You're resuming from a checkpoint that ended mid-epoch.\"\n",
      "Restored all states from the checkpoint file at metric_logs/test_trashbin_v1/version_0/checkpoints/epoch=9-step=3299.ckpt\n",
      "\n",
      "  | Name          | Type              | Params\n",
      "----------------------------------------------------\n",
      "0 | embedding_net | SqueezeNet        | 735 K \n",
      "1 | criterion     | TripletMarginLoss | 0     \n",
      "----------------------------------------------------\n",
      "735 K     Trainable params\n",
      "0         Non-trainable params\n",
      "735 K     Total params\n",
      "2.942     Total estimated model params size (MB)\n",
      "/Users/danilo/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /Users/danilo/GitHub/deep-learning/metric_logs/test_trashbin_v1/version_0 exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilo/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:117: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1996\n",
      "/Users/danilo/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:117: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:   0%|          | 0/413 [00:00<?, ?it/s]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   0%|          | 1/413 [00:15<1:48:54, 15.86s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   0%|          | 2/413 [00:28<1:36:11, 14.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   1%|          | 3/413 [00:40<1:31:59, 13.46s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   1%|          | 4/413 [00:53<1:30:21, 13.26s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   1%|          | 5/413 [01:05<1:29:29, 13.16s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   1%|▏         | 6/413 [01:18<1:28:26, 13.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   2%|▏         | 7/413 [01:30<1:27:21, 12.91s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   2%|▏         | 8/413 [01:42<1:26:34, 12.83s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   2%|▏         | 9/413 [01:54<1:25:59, 12.77s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   2%|▏         | 10/413 [02:06<1:25:12, 12.69s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   3%|▎         | 11/413 [02:19<1:24:43, 12.65s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   3%|▎         | 12/413 [02:31<1:24:13, 12.60s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   3%|▎         | 13/413 [02:43<1:23:53, 12.58s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   3%|▎         | 14/413 [02:55<1:23:33, 12.56s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   4%|▎         | 15/413 [03:07<1:23:08, 12.53s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   4%|▍         | 16/413 [03:19<1:22:41, 12.50s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   4%|▍         | 17/413 [03:32<1:22:20, 12.48s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   4%|▍         | 18/413 [03:44<1:21:57, 12.45s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   5%|▍         | 19/413 [03:56<1:21:37, 12.43s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   5%|▍         | 20/413 [04:08<1:21:16, 12.41s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   5%|▌         | 21/413 [04:20<1:20:57, 12.39s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   5%|▌         | 22/413 [04:32<1:20:37, 12.37s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   6%|▌         | 23/413 [04:44<1:20:20, 12.36s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   6%|▌         | 24/413 [04:56<1:20:02, 12.34s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   6%|▌         | 25/413 [05:08<1:19:43, 12.33s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   6%|▋         | 26/413 [05:20<1:19:25, 12.31s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   7%|▋         | 27/413 [05:32<1:19:08, 12.30s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   7%|▋         | 28/413 [05:44<1:18:51, 12.29s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   7%|▋         | 29/413 [05:56<1:18:37, 12.29s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   7%|▋         | 30/413 [06:08<1:18:21, 12.28s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   8%|▊         | 31/413 [06:20<1:18:05, 12.26s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   8%|▊         | 32/413 [06:32<1:17:50, 12.26s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   8%|▊         | 33/413 [06:44<1:17:35, 12.25s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   8%|▊         | 34/413 [06:56<1:17:21, 12.25s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   8%|▊         | 35/413 [07:08<1:17:06, 12.24s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   9%|▊         | 36/413 [07:20<1:16:50, 12.23s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   9%|▉         | 37/413 [07:32<1:16:35, 12.22s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   9%|▉         | 38/413 [07:44<1:16:20, 12.21s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:   9%|▉         | 39/413 [07:56<1:16:05, 12.21s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  10%|▉         | 40/413 [08:08<1:15:52, 12.21s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  10%|▉         | 41/413 [08:20<1:15:40, 12.21s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  10%|█         | 42/413 [08:32<1:15:29, 12.21s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  10%|█         | 43/413 [08:44<1:15:16, 12.21s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  11%|█         | 44/413 [08:56<1:15:03, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  11%|█         | 45/413 [09:09<1:14:49, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  11%|█         | 46/413 [09:21<1:14:37, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  11%|█▏        | 47/413 [09:33<1:14:25, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  12%|█▏        | 48/413 [09:45<1:14:12, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  12%|█▏        | 49/413 [09:57<1:13:59, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  12%|█▏        | 50/413 [10:10<1:13:49, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  12%|█▏        | 51/413 [10:22<1:13:36, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  13%|█▎        | 52/413 [10:34<1:13:24, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  13%|█▎        | 53/413 [10:46<1:13:13, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  13%|█▎        | 54/413 [10:59<1:13:01, 12.21s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  13%|█▎        | 55/413 [11:11<1:12:49, 12.21s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  14%|█▎        | 56/413 [11:23<1:12:36, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  14%|█▍        | 57/413 [11:35<1:12:23, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  14%|█▍        | 58/413 [11:47<1:12:09, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  14%|█▍        | 59/413 [11:59<1:11:58, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  15%|█▍        | 60/413 [12:12<1:11:46, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  15%|█▍        | 61/413 [12:24<1:11:35, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  15%|█▌        | 62/413 [12:36<1:11:22, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  15%|█▌        | 63/413 [12:48<1:11:09, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  15%|█▌        | 64/413 [13:00<1:10:56, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  16%|█▌        | 65/413 [13:12<1:10:43, 12.19s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  16%|█▌        | 66/413 [13:24<1:10:30, 12.19s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  16%|█▌        | 67/413 [13:37<1:10:19, 12.19s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  16%|█▋        | 68/413 [13:49<1:10:08, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  17%|█▋        | 69/413 [14:01<1:09:57, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  17%|█▋        | 70/413 [14:14<1:09:45, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  17%|█▋        | 71/413 [14:26<1:09:32, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  17%|█▋        | 72/413 [14:38<1:09:20, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  18%|█▊        | 73/413 [14:50<1:09:06, 12.20s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  18%|█▊        | 74/413 [15:02<1:08:54, 12.19s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  18%|█▊        | 75/413 [15:14<1:08:40, 12.19s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  18%|█▊        | 76/413 [15:26<1:08:28, 12.19s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  19%|█▊        | 77/413 [15:38<1:08:16, 12.19s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  19%|█▉        | 78/413 [15:50<1:08:04, 12.19s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  19%|█▉        | 79/413 [16:03<1:07:52, 12.19s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  19%|█▉        | 80/413 [16:15<1:07:39, 12.19s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  20%|█▉        | 81/413 [16:27<1:07:27, 12.19s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  20%|█▉        | 82/413 [16:39<1:07:14, 12.19s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  20%|██        | 83/413 [16:51<1:07:02, 12.19s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  20%|██        | 84/413 [17:04<1:06:50, 12.19s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  21%|██        | 85/413 [17:15<1:06:37, 12.19s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  21%|██        | 86/413 [17:27<1:06:24, 12.18s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  21%|██        | 87/413 [17:39<1:06:10, 12.18s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  21%|██▏       | 88/413 [17:51<1:05:57, 12.18s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  22%|██▏       | 89/413 [18:03<1:05:44, 12.17s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  22%|██▏       | 90/413 [18:15<1:05:30, 12.17s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  22%|██▏       | 91/413 [18:27<1:05:17, 12.17s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  22%|██▏       | 92/413 [18:39<1:05:04, 12.16s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  23%|██▎       | 93/413 [18:50<1:04:51, 12.16s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  23%|██▎       | 94/413 [19:02<1:04:37, 12.16s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  23%|██▎       | 95/413 [19:14<1:04:24, 12.15s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  23%|██▎       | 96/413 [19:26<1:04:11, 12.15s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  23%|██▎       | 97/413 [19:38<1:03:58, 12.15s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  24%|██▎       | 98/413 [19:50<1:03:45, 12.15s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  24%|██▍       | 99/413 [20:02<1:03:32, 12.14s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  24%|██▍       | 100/413 [20:14<1:03:20, 12.14s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  24%|██▍       | 101/413 [20:26<1:03:09, 12.14s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  25%|██▍       | 102/413 [20:38<1:02:56, 12.14s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  25%|██▍       | 103/413 [20:51<1:02:45, 12.15s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  25%|██▌       | 104/413 [21:03<1:02:34, 12.15s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  25%|██▌       | 105/413 [21:15<1:02:22, 12.15s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  26%|██▌       | 106/413 [21:27<1:02:09, 12.15s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  26%|██▌       | 107/413 [21:39<1:01:57, 12.15s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  26%|██▌       | 108/413 [21:51<1:01:44, 12.15s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  26%|██▋       | 109/413 [22:03<1:01:31, 12.14s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  27%|██▋       | 110/413 [22:15<1:01:18, 12.14s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  27%|██▋       | 111/413 [22:27<1:01:05, 12.14s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  27%|██▋       | 112/413 [22:39<1:00:53, 12.14s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  27%|██▋       | 113/413 [22:51<1:00:41, 12.14s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  28%|██▊       | 114/413 [23:03<1:00:29, 12.14s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  28%|██▊       | 115/413 [23:15<1:00:16, 12.14s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  28%|██▊       | 116/413 [23:27<1:00:03, 12.13s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  28%|██▊       | 117/413 [23:39<59:50, 12.13s/it, loss=2, v_num=1]  loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  29%|██▊       | 118/413 [23:51<59:37, 12.13s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  29%|██▉       | 119/413 [24:02<59:25, 12.13s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  29%|██▉       | 120/413 [24:14<59:12, 12.12s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  29%|██▉       | 121/413 [24:27<59:00, 12.13s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  30%|██▉       | 122/413 [24:39<58:49, 12.13s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  30%|██▉       | 123/413 [24:51<58:36, 12.13s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  30%|███       | 124/413 [25:03<58:24, 12.13s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  30%|███       | 125/413 [25:15<58:12, 12.13s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  31%|███       | 126/413 [25:27<57:59, 12.12s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  31%|███       | 127/413 [25:40<57:48, 12.13s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  31%|███       | 128/413 [25:52<57:35, 12.13s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  31%|███       | 129/413 [26:04<57:24, 12.13s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  31%|███▏      | 130/413 [26:16<57:12, 12.13s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  32%|███▏      | 131/413 [26:28<56:59, 12.13s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  32%|███▏      | 132/413 [26:40<56:47, 12.13s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  32%|███▏      | 133/413 [26:52<56:35, 12.13s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  32%|███▏      | 134/413 [27:04<56:22, 12.12s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  33%|███▎      | 135/413 [27:16<56:10, 12.12s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  33%|███▎      | 136/413 [27:28<55:57, 12.12s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  33%|███▎      | 137/413 [27:40<55:45, 12.12s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  33%|███▎      | 138/413 [27:52<55:32, 12.12s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  34%|███▎      | 139/413 [28:04<55:20, 12.12s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  34%|███▍      | 140/413 [28:16<55:07, 12.12s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  34%|███▍      | 141/413 [28:28<54:55, 12.12s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  34%|███▍      | 142/413 [28:40<54:42, 12.11s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  35%|███▍      | 143/413 [28:52<54:30, 12.11s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  35%|███▍      | 144/413 [29:03<54:17, 12.11s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  35%|███▌      | 145/413 [29:15<54:05, 12.11s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  35%|███▌      | 146/413 [29:27<53:52, 12.11s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  36%|███▌      | 147/413 [29:39<53:40, 12.11s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  36%|███▌      | 148/413 [29:51<53:28, 12.11s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  36%|███▌      | 149/413 [30:03<53:15, 12.10s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  36%|███▋      | 150/413 [30:15<53:03, 12.10s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  37%|███▋      | 151/413 [30:27<52:51, 12.10s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  37%|███▋      | 152/413 [30:39<52:38, 12.10s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  37%|███▋      | 153/413 [30:51<52:26, 12.10s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  37%|███▋      | 154/413 [31:03<52:13, 12.10s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  38%|███▊      | 155/413 [31:15<52:01, 12.10s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  38%|███▊      | 156/413 [31:27<51:49, 12.10s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  38%|███▊      | 157/413 [31:38<51:35, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  38%|███▊      | 158/413 [31:50<51:23, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  38%|███▊      | 159/413 [32:02<51:10, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  39%|███▊      | 160/413 [32:14<50:58, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  39%|███▉      | 161/413 [32:26<50:46, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  39%|███▉      | 162/413 [32:38<50:33, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  39%|███▉      | 163/413 [32:49<50:21, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  40%|███▉      | 164/413 [33:01<50:09, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  40%|███▉      | 165/413 [33:13<49:56, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  40%|████      | 166/413 [33:25<49:44, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  40%|████      | 167/413 [33:37<49:32, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  41%|████      | 168/413 [33:49<49:19, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  41%|████      | 169/413 [34:02<49:08, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  41%|████      | 170/413 [34:14<48:56, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  41%|████▏     | 171/413 [34:26<48:44, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  42%|████▏     | 172/413 [34:38<48:32, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  42%|████▏     | 173/413 [34:50<48:19, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  42%|████▏     | 174/413 [35:02<48:07, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  42%|████▏     | 175/413 [35:14<47:55, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  43%|████▎     | 176/413 [35:26<47:43, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  43%|████▎     | 177/413 [35:38<47:31, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  43%|████▎     | 178/413 [35:51<47:20, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  43%|████▎     | 179/413 [36:04<47:09, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  44%|████▎     | 180/413 [36:16<46:57, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  44%|████▍     | 181/413 [36:28<46:45, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  44%|████▍     | 182/413 [36:40<46:32, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  44%|████▍     | 183/413 [36:52<46:20, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  45%|████▍     | 184/413 [37:04<46:08, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  45%|████▍     | 185/413 [37:16<45:55, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  45%|████▌     | 186/413 [37:27<45:43, 12.09s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  45%|████▌     | 187/413 [37:39<45:31, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  46%|████▌     | 188/413 [37:51<45:18, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  46%|████▌     | 189/413 [38:03<45:06, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  46%|████▌     | 190/413 [38:15<44:54, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  46%|████▌     | 191/413 [38:27<44:42, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  46%|████▋     | 192/413 [38:39<44:29, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  47%|████▋     | 193/413 [38:51<44:17, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  47%|████▋     | 194/413 [39:03<44:05, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  47%|████▋     | 195/413 [39:15<43:52, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  47%|████▋     | 196/413 [39:27<43:40, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  48%|████▊     | 197/413 [39:38<43:28, 12.08s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  48%|████▊     | 198/413 [39:50<43:16, 12.07s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  48%|████▊     | 199/413 [40:02<43:03, 12.07s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  48%|████▊     | 200/413 [40:14<42:51, 12.07s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  49%|████▊     | 201/413 [40:26<42:39, 12.07s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  49%|████▉     | 202/413 [40:38<42:26, 12.07s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  49%|████▉     | 203/413 [40:50<42:14, 12.07s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  49%|████▉     | 204/413 [41:02<42:02, 12.07s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  50%|████▉     | 205/413 [41:13<41:50, 12.07s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  50%|████▉     | 206/413 [41:25<41:37, 12.07s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  50%|█████     | 207/413 [41:37<41:25, 12.07s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  50%|█████     | 208/413 [41:49<41:13, 12.06s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  51%|█████     | 209/413 [42:01<41:00, 12.06s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  51%|█████     | 210/413 [42:13<40:48, 12.06s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  51%|█████     | 211/413 [42:24<40:36, 12.06s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  51%|█████▏    | 212/413 [42:36<40:24, 12.06s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  52%|█████▏    | 213/413 [42:48<40:11, 12.06s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  52%|█████▏    | 214/413 [43:00<39:59, 12.06s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  52%|█████▏    | 215/413 [43:12<39:47, 12.06s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  52%|█████▏    | 216/413 [43:24<39:35, 12.06s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  53%|█████▎    | 217/413 [43:35<39:22, 12.05s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  53%|█████▎    | 218/413 [43:47<39:10, 12.05s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  53%|█████▎    | 219/413 [43:59<38:58, 12.05s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  53%|█████▎    | 220/413 [44:11<38:46, 12.05s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  54%|█████▎    | 221/413 [44:23<38:33, 12.05s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  54%|█████▍    | 222/413 [44:35<38:21, 12.05s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  54%|█████▍    | 223/413 [44:46<38:09, 12.05s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  54%|█████▍    | 224/413 [44:58<37:57, 12.05s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  54%|█████▍    | 225/413 [45:10<37:44, 12.05s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  55%|█████▍    | 226/413 [45:22<37:32, 12.05s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  55%|█████▍    | 227/413 [45:34<37:20, 12.05s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  55%|█████▌    | 228/413 [45:46<37:08, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  55%|█████▌    | 229/413 [45:57<36:56, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  56%|█████▌    | 230/413 [46:09<36:43, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  56%|█████▌    | 231/413 [46:21<36:31, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  56%|█████▌    | 232/413 [46:33<36:19, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  56%|█████▋    | 233/413 [46:45<36:07, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  57%|█████▋    | 234/413 [46:57<35:55, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  57%|█████▋    | 235/413 [47:09<35:43, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  57%|█████▋    | 236/413 [47:21<35:31, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  57%|█████▋    | 237/413 [47:34<35:19, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  58%|█████▊    | 238/413 [47:46<35:07, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  58%|█████▊    | 239/413 [47:58<34:55, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  58%|█████▊    | 240/413 [48:10<34:43, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  58%|█████▊    | 241/413 [48:21<34:31, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  59%|█████▊    | 242/413 [48:33<34:18, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  59%|█████▉    | 243/413 [48:45<34:06, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  59%|█████▉    | 244/413 [48:57<33:54, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  59%|█████▉    | 245/413 [49:09<33:42, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  60%|█████▉    | 246/413 [49:21<33:30, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  60%|█████▉    | 247/413 [49:33<33:18, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  60%|██████    | 248/413 [49:45<33:06, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  60%|██████    | 249/413 [49:57<32:53, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  61%|██████    | 250/413 [50:08<32:41, 12.04s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  61%|██████    | 251/413 [50:20<32:29, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  61%|██████    | 252/413 [50:32<32:17, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  61%|██████▏   | 253/413 [50:44<32:05, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  62%|██████▏   | 254/413 [50:56<31:53, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  62%|██████▏   | 255/413 [51:08<31:41, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  62%|██████▏   | 256/413 [51:20<31:28, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  62%|██████▏   | 257/413 [51:31<31:16, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  62%|██████▏   | 258/413 [51:43<31:04, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  63%|██████▎   | 259/413 [51:55<30:52, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  63%|██████▎   | 260/413 [52:07<30:40, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  63%|██████▎   | 261/413 [52:19<30:28, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  63%|██████▎   | 262/413 [52:31<30:16, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  64%|██████▎   | 263/413 [52:43<30:04, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  64%|██████▍   | 264/413 [52:55<29:51, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  64%|██████▍   | 265/413 [53:06<29:39, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  64%|██████▍   | 266/413 [53:18<29:27, 12.03s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  65%|██████▍   | 267/413 [53:30<29:15, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  65%|██████▍   | 268/413 [53:42<29:03, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  65%|██████▌   | 269/413 [53:54<28:51, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  65%|██████▌   | 270/413 [54:06<28:39, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  66%|██████▌   | 271/413 [54:18<28:27, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  66%|██████▌   | 272/413 [54:30<28:15, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  66%|██████▌   | 273/413 [54:42<28:03, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  66%|██████▋   | 274/413 [54:53<27:51, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  67%|██████▋   | 275/413 [55:05<27:38, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  67%|██████▋   | 276/413 [55:17<27:26, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  67%|██████▋   | 277/413 [55:29<27:14, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  67%|██████▋   | 278/413 [55:41<27:02, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  68%|██████▊   | 279/413 [55:53<26:50, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  68%|██████▊   | 280/413 [56:05<26:38, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  68%|██████▊   | 281/413 [56:17<26:26, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  68%|██████▊   | 282/413 [56:28<26:14, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  69%|██████▊   | 283/413 [56:40<26:02, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  69%|██████▉   | 284/413 [56:52<25:50, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  69%|██████▉   | 285/413 [57:04<25:38, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  69%|██████▉   | 286/413 [57:16<25:26, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  69%|██████▉   | 287/413 [57:28<25:13, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  70%|██████▉   | 288/413 [57:40<25:01, 12.02s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  70%|██████▉   | 289/413 [57:52<24:49, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  70%|███████   | 290/413 [58:03<24:37, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  70%|███████   | 291/413 [58:15<24:25, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  71%|███████   | 292/413 [58:27<24:13, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  71%|███████   | 293/413 [58:39<24:01, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  71%|███████   | 294/413 [58:51<23:49, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  71%|███████▏  | 295/413 [59:03<23:37, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  72%|███████▏  | 296/413 [59:15<23:25, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  72%|███████▏  | 297/413 [59:27<23:13, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  72%|███████▏  | 298/413 [59:39<23:01, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  72%|███████▏  | 299/413 [59:50<22:49, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  73%|███████▎  | 300/413 [1:00:02<22:37, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  73%|███████▎  | 301/413 [1:00:14<22:25, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  73%|███████▎  | 302/413 [1:00:26<22:12, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  73%|███████▎  | 303/413 [1:00:38<22:00, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  74%|███████▎  | 304/413 [1:00:50<21:48, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  74%|███████▍  | 305/413 [1:01:02<21:36, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  74%|███████▍  | 306/413 [1:01:14<21:24, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  74%|███████▍  | 307/413 [1:01:25<21:12, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  75%|███████▍  | 308/413 [1:01:37<21:00, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  75%|███████▍  | 309/413 [1:01:49<20:48, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  75%|███████▌  | 310/413 [1:02:01<20:36, 12.01s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  75%|███████▌  | 311/413 [1:02:13<20:24, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  76%|███████▌  | 312/413 [1:02:25<20:12, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  76%|███████▌  | 313/413 [1:02:37<20:00, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  76%|███████▌  | 314/413 [1:02:49<19:48, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  76%|███████▋  | 315/413 [1:03:01<19:36, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  77%|███████▋  | 316/413 [1:03:12<19:24, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  77%|███████▋  | 317/413 [1:03:24<19:12, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  77%|███████▋  | 318/413 [1:03:36<19:00, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  77%|███████▋  | 319/413 [1:03:48<18:48, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  77%|███████▋  | 320/413 [1:04:00<18:36, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  78%|███████▊  | 321/413 [1:04:12<18:24, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  78%|███████▊  | 322/413 [1:04:24<18:12, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  78%|███████▊  | 323/413 [1:04:36<18:00, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  78%|███████▊  | 324/413 [1:04:48<17:48, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  79%|███████▊  | 325/413 [1:05:00<17:36, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  79%|███████▉  | 326/413 [1:05:12<17:24, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  79%|███████▉  | 327/413 [1:05:24<17:12, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  79%|███████▉  | 328/413 [1:05:35<16:59, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10:  80%|███████▉  | 329/413 [1:05:47<16:47, 12.00s/it, loss=2, v_num=1]loss embedd 0.0\n",
      "loss 2.0\n",
      "Epoch 10: 100%|██████████| 413/413 [1:14:42<00:00, 10.85s/it, loss=2, v_num=1]"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "ModelCheckpoint(monitor='val_loss') not found in the returned metrics: ['train/loss', 'valid/loss']. HINT: Did you call self.log('val_loss', value) in the LightningModule?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xc/1c23sj2s5h7d869ldsyfkyk40000gn/T/ipykernel_5420/1487178221.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorBoardLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"metric_logs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test_trashbin_v1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGPUS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_val_every_n_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtriplet_trashbin_task_v2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriplet_dataset_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtriplet_dataset_test_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'metric_logs/test_trashbin_v1/version_0/checkpoints/epoch=9-step=3299.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, train_dataloader, ckpt_path)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mtrain_dataloaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         self._call_and_handle_interrupt(\n\u001b[0;32m--> 739\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m         )\n\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    681\u001b[0m         \"\"\"\n\u001b[1;32m    682\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m         \u001b[0;31m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;31m# TODO: ckpt_path only in v1.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0mckpt_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mckpt_path\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 773\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0;31m# dispatch `start_training` or `start_evaluating` or `start_predicting`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1195\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;31m# plugin will finalized fitting (e.g. ddp_spawn will load trained model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1273\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_predicting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_type_plugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/plugins/training_type/training_type_plugin.py\u001b[0m in \u001b[0;36mstart_training\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# double dispatch to initiate the training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstart_evaluating\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"pl.Trainer\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1313\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_detect_anomaly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_detect_anomaly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1315\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_EVALUATE_OUTPUT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestarting\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/loops/fit_loop.py\u001b[0m in \u001b[0;36madvance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_training_epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_fetcher\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;31m# the global step is manually decreased here due to backwards compatibility with existing loggers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/loops/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_run_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py\u001b[0m in \u001b[0;36mon_run_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;31m# call train epoch end hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_epoch_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_epoch_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger_connector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mcall_hook\u001b[0;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1489\u001b[0m             \u001b[0mcallback_fx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1490\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback_fx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1491\u001b[0;31m                 \u001b[0mcallback_fx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1493\u001b[0m             \u001b[0;31m# next call hook in lightningModule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/callback_hook.py\u001b[0m in \u001b[0;36mon_train_epoch_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;34m\"\"\"Called when the epoch ends.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_validation_epoch_start\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36mon_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_every_n_epochs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         ):\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_loop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36msave_checkpoint\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0mglobal_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_monitor_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0;31m# track epoch when ckpt was last checked\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py\u001b[0m in \u001b[0;36m_validate_monitor_key\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[0mwarning_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mMisconfigurationException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     def _get_metric_interpolated_filepath_name(\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: ModelCheckpoint(monitor='val_loss') not found in the returned metrics: ['train/loss', 'valid/loss']. HINT: Did you call self.log('val_loss', value) in the LightningModule?"
     ]
    }
   ],
   "source": [
    "triplet_trashbin_task_v2 = TripletNetworkTask(squeezeNet_1_0, lr=0.002)\n",
    "\n",
    "checkpoint_callback = [ ModelCheckpoint(\n",
    "    monitor= 'valid/loss',\n",
    "    dirpath='/Users/danilo/GitHub/deep-learning/metric_logs/test_trashbin_v1/version_0/',\n",
    "    filename='epoch=9-step=3299'\n",
    ") ]\n",
    "\n",
    "logger = TensorBoardLogger(\"metric_logs\", name=\"test_trashbin_v1\",)\n",
    "trainer = pl.Trainer(gpus=GPUS, logger = logger, max_epochs = 15, check_val_every_n_epoch = 1, callbacks=checkpoint_callback )\n",
    "trainer.fit(triplet_trashbin_task_v2, triplet_dataset_train_loader, triplet_dataset_test_loader, ckpt_path='metric_logs/test_trashbin_v1/version_0/checkpoints/epoch=9-step=3299.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: controlla i commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: devo estrarre TSNE??\n",
    "# Come continuo ???"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b5afb9c6c69ce826c7b3420d962c361055e44e7f6b101c54e3065067bcff4ff"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
