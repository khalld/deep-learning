{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Wcbq1RVx6NFK",
        "outputId": "2ac938df-fe5a-4417-cd8f-2fc01b464e9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "f8fmJPUh6P7_",
        "outputId": "33854261-9865-41c7-e696-7cbb26fbdce6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 30 12:15:19 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8    26W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Vwyqzg436NFM",
        "outputId": "03e7553b-62d6-448b-e991-aa383191107c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 527 kB 4.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 52.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 952 kB 43.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 829 kB 39.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 396 kB 48.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 41.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 45.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 50.3 MB/s \n",
            "\u001b[?25h  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 85.5 MB 96 kB/s \n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet pytorch-lightning\n",
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fOWzOW-a6NFM"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import faiss\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch.utils import data # necessary to create a map-style dataset https://pytorch.org/docs/stable/data.html\n",
        "from os.path import splitext, join\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from torchvision import transforms\n",
        "from torch.optim import SGD\n",
        "from torch import nn\n",
        "from tqdm import tqdm\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yzh4S5Gm6NFN"
      },
      "source": [
        "### Dataset classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "G8wJUP9P6NFN"
      },
      "outputs": [],
      "source": [
        "class TrashbinDataset(data.Dataset): # data.Dataset https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset\n",
        "    \"\"\" A map-style dataset class used to manipulate a dataset composed by:\n",
        "        image path of trashbin and associated label that describe the available capacity of the trashbin\n",
        "            0 : empty trashbin\n",
        "            1 : half trashbin\n",
        "            2 : full trashbin\n",
        "\n",
        "        Attributes\n",
        "        ----------\n",
        "        data : str\n",
        "            path of csv file\n",
        "        transform : torchvision.transforms\n",
        "\n",
        "        Methods\n",
        "        -------\n",
        "        __len__()\n",
        "            Return the length of the dataset\n",
        "\n",
        "        __getitem__(i)\n",
        "            Return image, label of i element of dataset  \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, csv: str=None, transform: transforms=None, path_gdrive: str=''):\n",
        "        \"\"\" Constructor of the dataset\n",
        "            Parameters\n",
        "            ----------\n",
        "            csv : str\n",
        "            path of the dataset\n",
        "\n",
        "            transform : torchvision.transforms\n",
        "            apply transform to the dataset\n",
        "\n",
        "            path_gdrive: str\n",
        "            necessary to apply the prepath in gdrive witouth changing csv\n",
        "\n",
        "            Raises\n",
        "            ------\n",
        "            NotImplementedError\n",
        "                If no path is passed is not provided a default dataset, default to load the image use only the csv file\n",
        "        \"\"\"\n",
        "        \n",
        "        if csv is None:\n",
        "            raise NotImplementedError(\"No default dataset is provided\")\n",
        "        if splitext(csv)[1] != '.csv':\n",
        "            raise NotImplementedError(\"Only .csv files are supported\")\n",
        "        \n",
        "        self.data = pd.read_csv(csv)        # import from csv using pandas\n",
        "        self.data = self.data.iloc[np.random.permutation(len(self.data))]       # random auto-permutation of the data\n",
        "        self.transform = transform\n",
        "        self.path_gdrive = path_gdrive\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\" Return length of dataset \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, i=None):\n",
        "        \"\"\" Return the i-th item of dataset\n",
        "\n",
        "            Parameters\n",
        "            ----------\n",
        "            i : int\n",
        "            i-th item of dataset\n",
        "\n",
        "            Raises\n",
        "            ------\n",
        "            NotImplementedError\n",
        "            If i is not a int\n",
        "        \"\"\"\n",
        "        if i is None:\n",
        "            raise NotImplementedError(\"Only int type is supported for get the item. None is not allowed\")\n",
        "        \n",
        "        im_path, im_label = self.data.iloc[i]['image'], self.data.iloc[i].label\n",
        "        im = Image.open(join(self.path_gdrive,im_path))        # Handle image with Image module from Pillow https://pillow.readthedocs.io/en/stable/reference/Image.html\n",
        "        if self.transform is not None:\n",
        "            im = self.transform(im)\n",
        "        return im, im_label\n",
        "\n",
        "class TripletTrashbin(data.Dataset):\n",
        "    def __init__(self, root = 'dataset/all_labels.csv', transform = None, path_gdrive='') -> None:\n",
        "#        super().__init__()\n",
        "        self.dataset = TrashbinDataset(root, transform=transform, path_gdrive=path_gdrive)\n",
        "        # self.dataset = self.dataset.data    # dipende dalla classe sopra, evito di chiamare un oggetto lungo\n",
        "        self.class_to_indices = [np.where(self.dataset.data.label == label)[0] for label in range(3)]  # N delle classi\n",
        "\n",
        "        self.generate_triplets()\n",
        "    \n",
        "    def generate_triplets(self):\n",
        "        \"\"\" Genera le triplete associando ongi elemento del dataset due nuovi elementi. Uno simile e uno dissimile\"\"\"\n",
        "\n",
        "        self.similar_idx = []\n",
        "        self.dissimilar_idx = []\n",
        "\n",
        "        # cu.printer_helper(\"Start making triplets...\")\n",
        "\n",
        "        for i in range(len(self.dataset)):\n",
        "            # classe del primo elemento della tripletta\n",
        "            c1 = self.dataset[i][1] # la classe la trovo sempre alla posizione 1 dato il dataset di sopra\n",
        "            # indice dell'elemento simile\n",
        "            j = np.random.choice(self.class_to_indices[c1])\n",
        "            # scelgo una classe diversa a caso\n",
        "            diff_class = np.random.choice(list(set(range(3))-{c1}))\n",
        "            # campiono dalla classe di ^ per ottenere l'indice dell'elemento dissimile\n",
        "            k = np.random.choice(self.class_to_indices[diff_class])\n",
        "\n",
        "            self.similar_idx.append(j)\n",
        "            self.dissimilar_idx.append(k)\n",
        "\n",
        "        # cu.printer_helper(\"Dataset loaded successfully!\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        im1, l1 = self.dataset[index]\n",
        "        im2, l2 = self.dataset[self.similar_idx[index]]\n",
        "        im3, l3 = self.dataset[self.dissimilar_idx[index]]\n",
        "\n",
        "        return im1, im2, im3, l1, l2, l3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RHLIBED6NFP"
      },
      "source": [
        "### Siamese Network classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KUm9M0qy6NFP"
      },
      "outputs": [],
      "source": [
        "class TripletNetworkTask(pl.LightningModule):\n",
        "    # lr uguale a quello del progetto vecchio\n",
        "    def __init__(self, embedding_net, lr=0.002, momentum=0.99, margin=2, num_class=3):\n",
        "        super(TripletNetworkTask, self).__init__()\n",
        "        self.save_hyperparameters()\n",
        "        self.embedding_net = embedding_net\n",
        "        self.criterion = nn.TripletMarginLoss(margin=margin)\n",
        "        self.num_class = num_class\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return SGD(self.embedding_net.parameters(), self.hparams.lr, momentum=self.hparams.momentum)\n",
        "\n",
        "    # Lightning automatically sets the model to training for training_step and to eval for validation.\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        I_i, I_j, I_k, *_ = batch\n",
        "        phi_i = self.embedding_net(I_i)\n",
        "        phi_j = self.embedding_net(I_j)\n",
        "        phi_k = self.embedding_net(I_k)\n",
        "\n",
        "        # calcoliamo la loss\n",
        "        loss_triplet = self.criterion(phi_i, phi_j, phi_k)\n",
        "        \n",
        "        loss_embedd = phi_i.norm(2) + phi_i.norm(2) + phi_i.norm(2)\n",
        "        loss = loss_triplet + 0.001 *loss_embedd\n",
        "\n",
        "        self.log('train/loss', loss)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        I_i, I_j, I_k, *_ = batch\n",
        "        phi_i = self.embedding_net(I_i)\n",
        "        phi_j = self.embedding_net(I_j)\n",
        "        phi_k = self.embedding_net(I_k)\n",
        "\n",
        "        #calcolo la loss\n",
        "        loss_triplet = self.criterion(phi_i, phi_j, phi_k)\n",
        "\n",
        "        loss_embedd = phi_i.norm(2) + phi_i.norm(2) + phi_i.norm(2)\n",
        "        loss = loss_triplet + 0.001 * loss_embedd\n",
        "\n",
        "        self.log('valid/loss', loss)\n",
        "        return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cn4MehYi6NFQ"
      },
      "source": [
        "### utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7Vvfy4Fb6NFQ"
      },
      "outputs": [],
      "source": [
        "def extract_rgb_representations(loader):\n",
        "    \"\"\" Baseline basata su nearest neighbor RGB\"\"\"\n",
        "    representations, labels = [], []\n",
        "    for batch in tqdm(loader, total=len(loader)):\n",
        "        representations.append(batch[0].view(batch[0].shape[0],-1).numpy())\n",
        "        labels.append(batch[1])\n",
        "\n",
        "    return np.concatenate(representations), np.concatenate(labels)\n",
        "\n",
        "def predict_nn(train_rep, test_rep, train_label):\n",
        "    \"\"\" Funzione che permette di predire le etichette sul test set analizzando l'algoritmo NN. !pip install faiss-gpu/cpu\"\"\"\n",
        "    # inizializzo l'oggetto index utilizzato x indicizzare le rappresentazioni\n",
        "    index = faiss.IndexFlat(train_rep.shape[1])\n",
        "    # aggiungo le rappresentazioni di training all'indice\n",
        "    index.add(train_rep.astype(np.float32))\n",
        "    # effettuiamo la ricerca\n",
        "\n",
        "    indices = np.array([index.search(x.reshape(1,-1).astype(np.float32), k=1)[1][0][0] for x in test_rep])\n",
        "\n",
        "    #restituisco le etichette predette\n",
        "    return train_label[indices].squeeze()\n",
        "\n",
        "def evaluate_classification(pred_label, ground_truth):\n",
        "    \"\"\" Valuto la bontà delle predizioni ottenute calcolando la distanza euclidea tra il vettore di label\n",
        "        predetto e quelli di ground truth\"\"\"\n",
        "    dist = np.sqrt(np.sum(np.square(pred_label-ground_truth)))\n",
        "    return dist\n",
        "\n",
        "def set_parameter_requires_grad(model, feature_extracting: bool):\n",
        "    \"\"\"Helper function that sets the `require_grad` attribute of parameter in the model to False when is used feature extracting\"\"\"\n",
        "\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "\n",
        "def extract_rep_squeezeNet(model, loader, device=\"cpu\"):\n",
        "    \"\"\" Il modello estrae vettori di rappresentazione di 1280 unità. definisco una funzione per estrarre le rappresentazioni di dataloader di training e test \"\"\"\n",
        "    \n",
        "    # Whenever you want to test your model you want to set it to model.eval() before which will disable dropout\n",
        "    # (and do the appropriate scaling of the weights), also it will make batchnorm work on the averages computed\n",
        "    # during training. Your code where you’ve commented model.eval() looks like like the right spot to set it to\n",
        "    # evaluation mode. Then after you simply do model.train() and you’ve enabled dropout, batchnorm to work as previously.\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    representations, labels = [], []\n",
        "    for batch in tqdm(loader, total=len(loader)):\n",
        "        x = batch[0].to(device)\n",
        "        rep = model(x)\n",
        "        rep = rep.detach().to('cpu').numpy()\n",
        "        labels.append(batch[1])\n",
        "        representations.append(rep)\n",
        "    \n",
        "    return np.concatenate(representations), np.concatenate(labels)\n",
        "\n",
        "def split_into_train_and_test(dataset, train_size_perc=0.8):\n",
        "    train_size = int(train_size_perc * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "\n",
        "    dataset_train, dataset_test = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    return dataset_train, dataset_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEMqYXNp6NFR"
      },
      "source": [
        "# Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwWeYdLe6NFR"
      },
      "source": [
        "Imposto i seed e le variabili globali:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hud1FO-r6NFR",
        "outputId": "abcf38d2-5c87-484c-ad45-5c592a0a21c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Global seed set to 1996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "random.seed(1996)\n",
        "np.random.seed(1996)\n",
        "pl.seed_everything(1996)\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uLM_xRkt6NFS"
      },
      "outputs": [],
      "source": [
        "PATH_DST = '/content/gdrive/MyDrive/trashbin-classifier/dataset/all_labels.csv'\n",
        "PATH_GDRIVE = '/content/gdrive/MyDrive/trashbin-classifier/'\n",
        "# TODO: se setto > 0 mi da \n",
        "# [W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
        "# e non mi permette di effettuare il training. tuttavia resta troppo lento. come procedo?\n",
        "NUM_WORKERS = 2\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 20\n",
        "GPUS = 1\n",
        "PRETRAINED_MODEL_PATH =  '/content/gdrive/MyDrive/trashbin-classifier/SqueezeNet1_1__lr=0.00282-50.pth'\n",
        "num_class = 3\n",
        "\n",
        "# valori pretrained\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225] "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx8CDyYp6NFT"
      },
      "source": [
        "Carico il dataset singolo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "w7if5pJX6NFT"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transf = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
        "\n",
        "dst = TrashbinDataset(csv=PATH_DST, transform=transf, path_gdrive=PATH_GDRIVE)\n",
        "\n",
        "dst_train, dst_test = split_into_train_and_test(dst)\n",
        "\n",
        "dst_train_loader = DataLoader(dst_train, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=True)\n",
        "dst_test_loader = DataLoader(dst_test, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n358LZZO6NFT"
      },
      "source": [
        "Estraggo le rappresentazioni rgb dai loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CTwmNsay6NFT",
        "outputId": "80519425-37d3-4e7a-b485-00d1dcd8bafc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 96%|█████████▌| 316/330 [1:05:04<02:52, 12.36s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    415\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/signal_handling.py\u001b[0m in \u001b[0;36mhandler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# Python can still get and update the process status successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0m_error_if_any_worker_fails\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mprevious_handler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 404) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3acdb6f72a81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdst_train_rep_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_train_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_rgb_representations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdst_train_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdst_test_rep_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdst_test_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_rgb_representations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdst_test_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-d8080481e3b6>\u001b[0m in \u001b[0;36mextract_rgb_representations\u001b[0;34m(loader)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\" Baseline basata su nearest neighbor RGB\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrepresentations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mrepresentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1186\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1001\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m                 \u001b[0mpids_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfailed_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1003\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DataLoader worker (pid(s) {}) exited unexpectedly'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpids_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1004\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmpty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 404) exited unexpectedly"
          ]
        }
      ],
      "source": [
        "dst_train_rep_rgb, dst_train_labels = extract_rgb_representations(loader=dst_train_loader)\n",
        "dst_test_rep_rgb, dst_test_labels = extract_rgb_representations(loader=dst_test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8OoSfNoF6NFU"
      },
      "source": [
        "Rappresentazioni di training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQgwCB9I6NFU"
      },
      "outputs": [],
      "source": [
        "dst_train_rep_rgb.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rn2e_l3Y6NFU"
      },
      "source": [
        "Ottengo le predizioni sul test-set usando `predict_nn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBwDPABZ6NFU"
      },
      "outputs": [],
      "source": [
        "pred_test_label_rgb = predict_nn(dst_train_rep_rgb, dst_test_rep_rgb, dst_train_labels)\n",
        "print(f\"Sample di label: {pred_test_label_rgb}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRARroLn6NFV"
      },
      "source": [
        "Valuto le performance della baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mN7yeP8l6NFV"
      },
      "outputs": [],
      "source": [
        "classification_error = evaluate_classification(pred_test_label_rgb, dst_test_labels)\n",
        "print(f\"Classification error: {classification_error:0.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeGqBOST6NFV"
      },
      "source": [
        "Importo per effettuare il training della triplenet il miglior modello trovato nella precedente relazione: `SqueezeNet v1.0`. Importo dunque i pesi già trovati dopo il training di 100 epoche .... <b>TODO migliora la descrizione</b> ... importo i pesi.. faccio le opportune modifiche ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riKeT5N46NFV"
      },
      "outputs": [],
      "source": [
        "# scarico il modello da pytorch\n",
        "squeezeNet_1_0 = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)\n",
        "# applico le opportune modifiche\n",
        "squeezeNet_1_0.classifier[1] = nn.Conv2d(512, num_class, kernel_size=(1,1), stride=(1,1))\n",
        "squeezeNet_1_0.num_classes = num_class\n",
        "# carico i pesi salvati\n",
        "squeezeNet_1_0.load_state_dict(torch.load(PRETRAINED_MODEL_PATH))\n",
        "\n",
        "# riduco l'ultimo modello alla funzione attività:\n",
        "squeezeNet_1_0.classifier = nn.Identity()\n",
        "squeezeNet_1_0(torch.zeros(1, 3, 224,224)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9535zb5I6NFW"
      },
      "outputs": [],
      "source": [
        "# uso il modello, allenato nel precedente progetto, per estrarre le rappresentazioni dal training e dal test set\n",
        "dst_train_rep, dst_train_labels = extract_rep_squeezeNet(squeezeNet_1_0, dst_train_loader)\n",
        "dst_test_rep, dst_test_labels = extract_rep_squeezeNet(squeezeNet_1_0, dst_test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyi2ieH66NFW"
      },
      "source": [
        "Valuto le performance del sistema:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAeTOFzA6NFW"
      },
      "outputs": [],
      "source": [
        "# valuto le performance del sistema con rappresentazioni non ancora ottimizzate\n",
        "pred_test_label = predict_nn(dst_train_rep, dst_test_rep, dst_train_labels)\n",
        "classification_error = evaluate_classification(pred_test_label, dst_test_labels)\n",
        "print(f\"Classification error: {classification_error}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vqbLWsm6NFW"
      },
      "source": [
        "Carico il dataset in triplette:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8aQVxvZw6NFX"
      },
      "outputs": [],
      "source": [
        "dst_triplet = TripletTrashbin(root=PATH_DST, transform=transf)\n",
        "\n",
        "dst_train_triplet, dst_test_triplet = split_into_train_and_test(dst_triplet)\n",
        "\n",
        "triplet_dataset_train_loader = DataLoader(dst_train_triplet, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=True)\n",
        "triplet_dataset_test_loader = DataLoader(dst_test_triplet, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caJBglp36NFX"
      },
      "outputs": [],
      "source": [
        "# TODO: mostra le immagini delle triplette"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vonBxtld6NFX"
      },
      "source": [
        "Alleno la rete con lr=0.002 che è il migliore trovato per SqueezeNet nel precedente progetto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSl7P8WE6NFX"
      },
      "outputs": [],
      "source": [
        "triplet_trashbin_task =  TripletNetworkTask(squeezeNet_1_0, lr=0.002)\n",
        "logger = TensorBoardLogger(\"metric_logs\", name=\"test_trashbin_v1\",)\n",
        "\n",
        "# TODO: salva ogni ...\n",
        "# TODO: CALLBACK!!!!!\n",
        "trainer = pl.Trainer(gpus=GPUS, logger = logger, max_epochs = 10, check_val_every_n_epoch=5, )\n",
        "trainer.fit(triplet_trashbin_task, triplet_dataset_train_loader, triplet_dataset_test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Juyz8UDd6NFX"
      },
      "outputs": [],
      "source": [
        "#TODO: devo estrarre TSNE??\n",
        "# Come continuo ???"
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "7b5afb9c6c69ce826c7b3420d962c361055e44e7f6b101c54e3065067bcff4ff"
    },
    "kernelspec": {
      "display_name": "Python 3.7.12 64-bit ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "main_colab.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}