{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet pytorch-lightning\n",
    "!pip install faiss-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import faiss\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from libs.code import *\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils import data # necessary to create a map-style dataset https://pytorch.org/docs/stable/data.html\n",
    "from os.path import splitext, join\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchvision import transforms\n",
    "from torch.optim import SGD\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrashbinDataset(data.Dataset): # data.Dataset https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset\n",
    "    \"\"\" A map-style dataset class used to manipulate a dataset composed by:\n",
    "        image path of trashbin and associated label that describe the available capacity of the trashbin\n",
    "            0 : empty trashbin\n",
    "            1 : half trashbin\n",
    "            2 : full trashbin\n",
    "\n",
    "        Attributes\n",
    "        ----------\n",
    "        data : str\n",
    "            path of csv file\n",
    "        transform : torchvision.transforms\n",
    "\n",
    "        Methods\n",
    "        -------\n",
    "        __len__()\n",
    "            Return the length of the dataset\n",
    "\n",
    "        __getitem__(i)\n",
    "            Return image, label of i element of dataset  \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv: str=None, transform: transforms=None, path_gdrive: str=''):\n",
    "        \"\"\" Constructor of the dataset\n",
    "            Parameters\n",
    "            ----------\n",
    "            csv : str\n",
    "            path of the dataset\n",
    "\n",
    "            transform : torchvision.transforms\n",
    "            apply transform to the dataset\n",
    "\n",
    "            path_gdrive: str\n",
    "            necessary to apply the prepath in gdrive witouth changing csv\n",
    "\n",
    "            Raises\n",
    "            ------\n",
    "            NotImplementedError\n",
    "                If no path is passed is not provided a default dataset, default to load the image use only the csv file\n",
    "        \"\"\"\n",
    "        \n",
    "        if csv is None:\n",
    "            raise NotImplementedError(\"No default dataset is provided\")\n",
    "        if splitext(csv)[1] != '.csv':\n",
    "            raise NotImplementedError(\"Only .csv files are supported\")\n",
    "        \n",
    "        self.data = pd.read_csv(csv)        # import from csv using pandas\n",
    "        self.data = self.data.iloc[np.random.permutation(len(self.data))]       # random auto-permutation of the data\n",
    "        self.transform = transform\n",
    "        self.path_gdrive = path_gdrive\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Return length of dataset \"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i=None):\n",
    "        \"\"\" Return the i-th item of dataset\n",
    "\n",
    "            Parameters\n",
    "            ----------\n",
    "            i : int\n",
    "            i-th item of dataset\n",
    "\n",
    "            Raises\n",
    "            ------\n",
    "            NotImplementedError\n",
    "            If i is not a int\n",
    "        \"\"\"\n",
    "        if i is None:\n",
    "            raise NotImplementedError(\"Only int type is supported for get the item. None is not allowed\")\n",
    "        \n",
    "        im_path, im_label = self.data.iloc[i]['image'], self.data.iloc[i].label\n",
    "        im = Image.open(join(self.path_gdrive,im_path))        # Handle image with Image module from Pillow https://pillow.readthedocs.io/en/stable/reference/Image.html\n",
    "        if self.transform is not None:\n",
    "            im = self.transform(im)\n",
    "        return im, im_label\n",
    "\n",
    "class TripletTrashbin(data.Dataset):\n",
    "    def __init__(self, root = 'dataset/all_labels.csv', transform = None, path_gdrive='') -> None:\n",
    "#        super().__init__()\n",
    "        self.dataset = TrashbinDataset(root, transform=transform, path_gdrive=path_gdrive)\n",
    "        # self.dataset = self.dataset.data    # dipende dalla classe sopra, evito di chiamare un oggetto lungo\n",
    "        self.class_to_indices = [np.where(self.dataset.data.label == label)[0] for label in range(3)]  # N delle classi\n",
    "\n",
    "        self.generate_triplets()\n",
    "    \n",
    "    def generate_triplets(self):\n",
    "        \"\"\" Genera le triplete associando ongi elemento del dataset due nuovi elementi. Uno simile e uno dissimile\"\"\"\n",
    "\n",
    "        self.similar_idx = []\n",
    "        self.dissimilar_idx = []\n",
    "\n",
    "        # cu.printer_helper(\"Start making triplets...\")\n",
    "\n",
    "        for i in range(len(self.dataset)):\n",
    "            # classe del primo elemento della tripletta\n",
    "            c1 = self.dataset[i][1] # la classe la trovo sempre alla posizione 1 dato il dataset di sopra\n",
    "            # indice dell'elemento simile\n",
    "            j = np.random.choice(self.class_to_indices[c1])\n",
    "            # scelgo una classe diversa a caso\n",
    "            diff_class = np.random.choice(list(set(range(3))-{c1}))\n",
    "            # campiono dalla classe di ^ per ottenere l'indice dell'elemento dissimile\n",
    "            k = np.random.choice(self.class_to_indices[diff_class])\n",
    "\n",
    "            self.similar_idx.append(j)\n",
    "            self.dissimilar_idx.append(k)\n",
    "\n",
    "        # cu.printer_helper(\"Dataset loaded successfully!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        im1, l1 = self.dataset[index]\n",
    "        im2, l2 = self.dataset[self.similar_idx[index]]\n",
    "        im3, l3 = self.dataset[self.dissimilar_idx[index]]\n",
    "\n",
    "        return im1, im2, im3, l1, l2, l3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Network classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletNetworkTask(pl.LightningModule):\n",
    "    # lr uguale a quello del progetto vecchio\n",
    "    def __init__(self, embedding_net, lr=0.002, momentum=0.99, margin=2, num_class=3):\n",
    "        super(TripletNetworkTask, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.embedding_net = embedding_net\n",
    "        self.criterion = nn.TripletMarginLoss(margin=margin)\n",
    "        self.num_class = num_class\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return SGD(self.embedding_net.parameters(), self.hparams.lr, momentum=self.hparams.momentum)\n",
    "\n",
    "    # Lightning automatically sets the model to training for training_step and to eval for validation.\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        I_i, I_j, I_k, *_ = batch\n",
    "        phi_i = self.embedding_net(I_i)\n",
    "        phi_j = self.embedding_net(I_j)\n",
    "        phi_k = self.embedding_net(I_k)\n",
    "\n",
    "        # calcoliamo la loss\n",
    "        loss_triplet = self.criterion(phi_i, phi_j, phi_k)\n",
    "        \n",
    "        loss_embedd = phi_i.norm(2) + phi_i.norm(2) + phi_i.norm(2)\n",
    "        loss = loss_triplet + 0.001 *loss_embedd\n",
    "\n",
    "        self.log('train/loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        I_i, I_j, I_k, *_ = batch\n",
    "        phi_i = self.embedding_net(I_i)\n",
    "        phi_j = self.embedding_net(I_j)\n",
    "        phi_k = self.embedding_net(I_k)\n",
    "\n",
    "        #calcolo la loss\n",
    "        loss_triplet = self.criterion(phi_i, phi_j, phi_k)\n",
    "\n",
    "        loss_embedd = phi_i.norm(2) + phi_i.norm(2) + phi_i.norm(2)\n",
    "        loss = loss_triplet + 0.001 * loss_embedd\n",
    "\n",
    "        self.log('valid/loss', loss)\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_rgb_representations(loader):\n",
    "    \"\"\" Baseline basata su nearest neighbor RGB\"\"\"\n",
    "    representations, labels = [], []\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        representations.append(batch[0].view(batch[0].shape[0],-1).numpy())\n",
    "        labels.append(batch[1])\n",
    "\n",
    "    return np.concatenate(representations), np.concatenate(labels)\n",
    "\n",
    "def predict_nn(train_rep, test_rep, train_label):\n",
    "    \"\"\" Funzione che permette di predire le etichette sul test set analizzando l'algoritmo NN. !pip install faiss-gpu/cpu\"\"\"\n",
    "    # inizializzo l'oggetto index utilizzato x indicizzare le rappresentazioni\n",
    "    index = faiss.IndexFlat(train_rep.shape[1])\n",
    "    # aggiungo le rappresentazioni di training all'indice\n",
    "    index.add(train_rep.astype(np.float32))\n",
    "    # effettuiamo la ricerca\n",
    "\n",
    "    indices = np.array([index.search(x.reshape(1,-1).astype(np.float32), k=1)[1][0][0] for x in test_rep])\n",
    "\n",
    "    #restituisco le etichette predette\n",
    "    return train_label[indices].squeeze()\n",
    "\n",
    "def evaluate_classification(pred_label, ground_truth):\n",
    "    \"\"\" Valuto la bontà delle predizioni ottenute calcolando la distanza euclidea tra il vettore di label\n",
    "        predetto e quelli di ground truth\"\"\"\n",
    "    dist = np.sqrt(np.sum(np.square(pred_label-ground_truth)))\n",
    "    return dist\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting: bool):\n",
    "    \"\"\"Helper function that sets the `require_grad` attribute of parameter in the model to False when is used feature extracting\"\"\"\n",
    "\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "def extract_rep_squeezeNet(model, loader, device=\"cpu\"):\n",
    "    \"\"\" Il modello estrae vettori di rappresentazione di 1280 unità. definisco una funzione per estrarre le rappresentazioni di dataloader di training e test \"\"\"\n",
    "    \n",
    "    # Whenever you want to test your model you want to set it to model.eval() before which will disable dropout\n",
    "    # (and do the appropriate scaling of the weights), also it will make batchnorm work on the averages computed\n",
    "    # during training. Your code where you’ve commented model.eval() looks like like the right spot to set it to\n",
    "    # evaluation mode. Then after you simply do model.train() and you’ve enabled dropout, batchnorm to work as previously.\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    representations, labels = [], []\n",
    "    for batch in tqdm(loader, total=len(loader)):\n",
    "        x = batch[0].to(device)\n",
    "        rep = model(x)\n",
    "        rep = rep.detach().to('cpu').numpy()\n",
    "        labels.append(batch[1])\n",
    "        representations.append(rep)\n",
    "    \n",
    "    return np.concatenate(representations), np.concatenate(labels)\n",
    "\n",
    "def split_into_train_and_test(dataset, train_size_perc=0.8):\n",
    "    train_size = int(train_size_perc * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "\n",
    "    dataset_train, dataset_test = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    return dataset_train, dataset_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imposto i seed e le variabili globali:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "random.seed(1996)\n",
    "np.random.seed(1996)\n",
    "pl.seed_everything(1996)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DST = 'dataset/all_labels.csv'\n",
    "PATH_GDRIVE = ''\n",
    "# TODO: se setto > 0 mi da \n",
    "# [W ParallelNative.cpp:214] Warning: Cannot set number of intraop threads after parallel work has started or after set_num_threads call when using native parallel backend (function set_num_threads)\n",
    "# e non mi permette di effettuare il training. tuttavia resta troppo lento. come procedo?\n",
    "NUM_WORKERS = 2\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "GPUS = 0\n",
    "PRETRAINED_MODEL_PATH =  'models/squeezeNet_pretrained.pth'\n",
    "num_class = 3\n",
    "\n",
    "# valori pretrained\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carico il dataset singolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transf = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor(), transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "dst = TrashbinDataset(csv=PATH_DST, transform=transf)\n",
    "\n",
    "dst_train, dst_test = split_into_train_and_test(dst)\n",
    "\n",
    "dst_train_loader = DataLoader(dst_train, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dst_test_loader = DataLoader(dst_test, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estraggo le rappresentazioni rgb dai loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330/330 [02:46<00:00,  1.98it/s]\n",
      "100%|██████████| 83/83 [00:41<00:00,  2.01it/s]\n"
     ]
    }
   ],
   "source": [
    "dst_train_rep_rgb, dst_train_labels = extract_rgb_representations(loader=dst_train_loader)\n",
    "dst_test_rep_rgb, dst_test_labels = extract_rgb_representations(loader=dst_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappresentazioni di training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10560, 150528)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dst_train_rep_rgb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ottengo le predizioni sul test-set usando `predict_nn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_label_rgb = predict_nn(dst_train_rep_rgb, dst_test_rep_rgb, dst_train_labels)\n",
    "print(f\"Sample di label: {pred_test_label_rgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valuto le performance della baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification error: 5.29\n"
     ]
    }
   ],
   "source": [
    "classification_error = evaluate_classification(pred_test_label_rgb, dst_test_labels)\n",
    "print(f\"Classification error: {classification_error:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importo per effettuare il training della triplenet il miglior modello trovato nella precedente relazione: `SqueezeNet v1.0`. Importo dunque i pesi già trovati dopo il training di 100 epoche .... <b>TODO migliora la descrizione</b> ... importo i pesi.. faccio le opportune modifiche ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/danilo/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 86528])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scarico il modello da pytorch\n",
    "squeezeNet_1_0 = torch.hub.load('pytorch/vision:v0.10.0', 'squeezenet1_0', pretrained=True)\n",
    "# applico le opportune modifiche\n",
    "squeezeNet_1_0.classifier[1] = nn.Conv2d(512, num_class, kernel_size=(1,1), stride=(1,1))\n",
    "squeezeNet_1_0.num_classes = num_class\n",
    "# carico i pesi salvati\n",
    "squeezeNet_1_0.load_state_dict(torch.load(PRETRAINED_MODEL_PATH))\n",
    "\n",
    "# riduco l'ultimo modello alla funzione attività:\n",
    "squeezeNet_1_0.classifier = nn.Identity()\n",
    "squeezeNet_1_0(torch.zeros(1, 3, 224,224)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330/330 [12:43<00:00,  2.31s/it]\n",
      "100%|██████████| 83/83 [03:09<00:00,  2.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# uso il modello, allenato nel precedente progetto, per estrarre le rappresentazioni dal training e dal test set\n",
    "dst_train_rep, dst_train_labels = extract_rep_squeezeNet(squeezeNet_1_0, dst_train_loader)\n",
    "dst_test_rep, dst_test_labels = extract_rep_squeezeNet(squeezeNet_1_0, dst_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valuto le performance del sistema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification error: 2.0\n"
     ]
    }
   ],
   "source": [
    "# valuto le performance del sistema con rappresentazioni non ancora ottimizzate\n",
    "pred_test_label = predict_nn(dst_train_rep, dst_test_rep, dst_train_labels)\n",
    "classification_error = evaluate_classification(pred_test_label, dst_test_labels)\n",
    "print(f\"Classification error: {classification_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carico il dataset in triplette:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dst_triplet = TripletTrashbin(root=PATH_DST, transform=transf)\n",
    "\n",
    "dst_train_triplet, dst_test_triplet = split_into_train_and_test(dst_triplet)\n",
    "\n",
    "triplet_dataset_train_loader = DataLoader(dst_train_triplet, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE, shuffle=True)\n",
    "triplet_dataset_test_loader = DataLoader(dst_test_triplet, num_workers=NUM_WORKERS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: mostra le immagini delle triplette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alleno la rete con lr=0.002 che è il migliore trovato per SqueezeNet nel precedente progetto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name          | Type              | Params\n",
      "----------------------------------------------------\n",
      "0 | embedding_net | SqueezeNet        | 735 K \n",
      "1 | criterion     | TripletMarginLoss | 0     \n",
      "----------------------------------------------------\n",
      "735 K     Trainable params\n",
      "0         Non-trainable params\n",
      "735 K     Total params\n",
      "2.942     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilo/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:117: UserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1996\n",
      "/Users/danilo/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/data_loading.py:117: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   3%|▎         | 9/330 [01:58<1:10:23, 13.16s/it, loss=nan, v_num=0]     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/danilo/GitHub/deep-learning/venv/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:686: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    }
   ],
   "source": [
    "triplet_trashbin_task =  TripletNetworkTask(squeezeNet_1_0, lr=0.002)\n",
    "logger = TensorBoardLogger(\"metric_logs\", name=\"test_trashbin_v1\",)\n",
    "\n",
    "# TODO: salva ogni ...\n",
    "# TODO: CALLBACK!!!!!\n",
    "trainer = pl.Trainer(gpus=GPUS, logger = logger, max_epochs = 10, check_val_every_n_epoch=5, )\n",
    "trainer.fit(triplet_trashbin_task, triplet_dataset_train_loader, triplet_dataset_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: devo estrarre TSNE??\n",
    "# Come continuo ???"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7b5afb9c6c69ce826c7b3420d962c361055e44e7f6b101c54e3065067bcff4ff"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
